{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "![img\\RAG.png](img/RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* documents_loader\n",
    "* splitter\n",
    "* vector_store\n",
    "* retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "doc = WebBaseLoader(\"https://lilianweng.github.io/posts/2021-09-25-train-large/\", \n",
    "                    bs_kwargs = dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))))\n",
    "docs = doc.load()\n",
    "\n",
    "spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "doc_splitted = spliter.split_documents(docs)\n",
    "vector_store = FAISS.from_documents(doc_splitted, embedding=OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Data Parallelism#\\nThe most naive way for Data parallelism (DP)  is to copy the same model weights into multiple workers and assign a fraction of data to each worker to be processed at the same time.\\nNaive DP cannot work well if the model size is larger than a single GPU node’s memory. Methods like GeePS (Cui et al. 2016) offload temporarily unused parameters back to CPU to work with limited GPU memory when the model is too big to fit into one machine. The data swapping transfer should happen at the backend and not interfere with training computation.\\nAt the end of each minibatch, workers need to synchronize gradients or weights to avoid staleness. There are two main synchronization approaches and both have clear pros & cons.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 1. Pseudo code for Pytorch DDP. (Image source: Li et al. 2021)\\nModel Parallelism#\\nModel parallelism (MP) aims to solve the case when the model weights cannot fit into a single node. The computation and model parameters are partitioned across multiple machines. Different from data parallelism where each worker hosts a full copy of the entire model, MP only allocates a fraction of model parameters on one worker and thus both the memory usage and the computation are reduced.\\nSince deep neural networks usually contain a stack of vertical layers, it feels straightforward to split a large model by layer, where a small consecutive set of layers are grouped into one partition on one worker. However, a naive implementation for running every data batch through multiple such workers with sequential dependency leads to big bubbles of waiting time and severe under-utilization of computation resources.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='The majority is occupied by model states, including optimizer states (e.g. Adam momentums and variances), gradients and parameters. Mixed-precision training demands a lot of memory since the optimizer needs to keep a copy of FP32 parameters and other optimizer states, besides the FP16 version.\\nThe remaining is consumed by activations, temporary buffers and unusable fragmented memory (named residual states in the paper).\\n\\nZeRO combines two approaches, ZeRO-DP and ZeRO-R.\\nZeRO-DP is an enhanced data parallelism to avoid simple redundancy over model states. It partitions optimizer state, gradients and parameters across multiple data parallel processes via a dynamic communication schedule to minimize the communication volume.\\nZeRO-R optimizes the memory consumption of residual states, using partitioned activation recomputation, constant buffer size and on-the-fly memory defragmentation.\\nCitation#\\nCited as:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='$$\\n1 - \\\\frac{2md}{(2m + 2(d-1))d} = \\\\frac{d-1}{m+d-1}\\n$$\\n\\nThe GPipe paper observed that the bubble overhead is almost negligible if the number of microbatches is more than 4x the number of partitions $m > 4d$ (when activation recomputation is applied).')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = retriever.invoke(\"What is DP?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = ({'context': retriever|format_docs, \"question\":RunnablePassthrough()}\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser()         \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DP stands for Data Parallelism, which involves copying the same model weights into multiple workers and assigning a fraction of data to each worker to be processed simultaneously. Naive DP may not work well if the model size exceeds a single GPU node's memory, leading to methods like GeePS that offload unused parameters back to the CPU. Workers in DP need to synchronize gradients or weights at the end of each minibatch to prevent staleness.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is DP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings   \n",
    "doc = WebBaseLoader(\"https://lilianweng.github.io/posts/2021-09-25-train-large/\",\n",
    "                    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))))\n",
    "docs = doc.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "doc_splitted = text_splitter.split_documents(docs)\n",
    "vector_store = FAISS.from_documents(doc_splitted, embedding=OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Query\n",
    "核心解决用户询问模糊的问题，通过生成多个prompts解决该问题\n",
    "\n",
    "![Multi Query](img/multi-query.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "template = \"\"\"\n",
    "        You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\n",
    "\n",
    "\"\"\"\n",
    "prompts_generate_query = PromptTemplate.from_template(\n",
    "    template = template,\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "generate_chains = prompts_generate_query|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_chains.invoke(\"What is DP?\") # 回复的是AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Can you explain the concept of DP?\\n2. How would you define DP?\\n3. What are the key characteristics of DP?\\n4. Could you provide an overview of DP?\\n5. In what context is DP commonly used?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "outputparser = StrOutputParser() \n",
    "resultsstring = outputparser.invoke(results)# AIMessage对象转为字符串\n",
    "resultsstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Can you explain the concept of DP?',\n",
       " '2. How would you define DP?',\n",
       " '3. What are the key characteristics of DP?',\n",
       " '4. Could you provide an overview of DP?',\n",
       " '5. In what context is DP commonly used?']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsstring.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.map() Return a new Runnable that maps a list of inputs to a list of outputs, 从接受单个input变成接受多个input\n",
    "retriever_map = retriever.map()\n",
    "results_all = retriever_map.invoke(resultsstring) # 为列表的列表 [[], [], []]最里面元素为Document对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import loads, dumps\n",
    "# dumps 返回一个对象的json字符串表示 loads 从 JSON 字符串恢复 LangChain 类。相当于加载（json.loads（文本））\n",
    "def get_unique_union(results_all: list[list]):\n",
    "    flattened_docs = [dumps(doc)for results in results_all for doc in results ]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Data Parallelism#\\nThe most naive way for Data parallelism (DP)  is to copy the same model weights into multiple workers and assign a fraction of data to each worker to be processed at the same time.\\nNaive DP cannot work well if the model size is larger than a single GPU node’s memory. Methods like GeePS (Cui et al. 2016) offload temporarily unused parameters back to CPU to work with limited GPU memory when the model is too big to fit into one machine. The data swapping transfer should happen at the backend and not interfere with training computation.\\nAt the end of each minibatch, workers need to synchronize gradients or weights to avoid staleness. There are two main synchronization approaches and both have clear pros & cons.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='$n$ feed-forward networks as experts $\\\\{E_i\\\\}^n_{i=1}$\\nA trainable gating network $G$ to learn a probability distribution over $n$ experts so as to route the traffic to a few selected experts.\\n\\nDepending on the gating outputs, not every expert has to be evaluated. When the number of experts is too large, we can consider using a two-level hierarchical MoE.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 3. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: Huang et al. 2019)\\nGPipe achieves almost linear speedup in throughput with the number of devices, although it is not always guaranteed if the model parameters are not evenly distributed across workers.\\nPipeDream (Narayanan et al. 2019) schedules each worker to alternatively process the forward and backward passes (1F1B).\\nPipeDream names each model partition “stage” and each stage worker can have multiple replicas to run data parallelism. In this process, PipeDream uses a deterministic round-robin load balancing strategy to assign work among multiple replicas of stages to ensure that the forward and backward passes for the same minibatch happen on the same replica.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Selective precision. They showed that selectively casting only a local part of the model to FP32 precision improves stability, while avoiding the expensive communication cost of FP32 tensors. The FP32 precision is only used within the body of the router function and the results are recast to FP16.\\nSmaller initialization. The initialization of weight matrices is sampled from a truncated normal distribution with mean $\\\\mu=0$ and stdev $\\\\sigma = \\\\sqrt{s/n}$. They also recommended reducing the transformer initialization scale parameter $s=1$ to $s=0.1$.\\nUse higher expert dropout. Fine-tuning often works with a small dataset. To avoid overfitting, the dropout rate within each expert is increased by a significant amount. Interestingly they found that increasing dropout across all layers lead to poor performance. In the paper, they used a dropout rate 0.1 at non-expert layers but 0.4 within expert FF layers.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Expert capacity: The amount of tokens going through one expert should not go above a threshold, named “expert capacity”. If a token is routed to experts that have reached their capacity, the token would be marked “overflowed” and the gating output is changed to a zero vector.\\nLocal group dispatching: Tokens are evenly partitioned into multiple local groups and the expert capacity is enforced on the group level.\\nAuxiliary loss: The motivation is similar to the original MoE aux loss. They add an auxiliary loss to minimize the mean square of the fraction of data routed to each expert.\\nRandom routing: The 2nd-best expert is selected with a probability proportional to its weight; otherwise, GShard follows a random routing, so as to add some randomness.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='where each entry $A[i,j]$ in $A \\\\in \\\\mathbb{R}^{e \\\\times n}$ marks whether the $i$-the expert selects the $j$-th token. Solving this is non-trivial. The paper used Dykstra’s algorithm that runs a sequence of multiple iterative computation steps. Capped expert choice results in a slight decrease in the fine-tuning performance in the experiments.\\nThe parameter $k$ is determined by $k=nc/e$, where $n$ is the total number of tokens in one batch and $c$ is a capacity factor indicating the average number of experts used by one token. The paper used $c=2$ in most experiments, but EC with $c=1$ still outperforms the top-1 token choice gating. Interestingly, $c=0.5$ only marginally hurts the training performance.\\nOne big drawback of EC is that it does not work when the batch size is too small, neither for auto-regressive text generation, because it needs to know the future tokens to do the top-$k$ selection.\\nOther Memory Saving Designs#\\nCPU Offloading#'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Their system Gist incorporates two encoding schemes:\\nLayer-specific lossless encoding; focus on ReLU-Pool (“Binarize”) and ReLU-Conv (“Sparse storage and dense computation”) patterns.\\nAggressive lossy encoding; use delayed precision reduction (DPR). They observed that the first immediate use of feature maps should be kept at high precision but the second use can tolerate lower precision.\\nThe experiments showed that Gist can reduce the memory cost by 2x across 5 SOTA image classification DNNs, with an average of 1.8x with only 4% performance overhead.\\nMemory Efficient Optimizer#\\nOptimizers are eager for memory consumption. Take the popular Adam optimizer as an example, it internally needs to maintain momentums and variances, both at the same scale as gradients and model parameters. All out of a sudden, we need to save 4x the memory of model weights.\\nSeveral optimizers have been proposed to reduce the memory footprint.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='$$\\n\\\\text{Attention}(X, Q, K, V) = \\\\text{softmax}(\\\\frac{(XQ) (XK)^\\\\top}{\\\\sqrt{d_k}}) XV\\n$$\\n\\n\\nFig. 8. Illustration of tensor parallelism for key transformer components proposed in Megatron-LM. (Image source: Shoeybi et al. 2020)\\nNarayanan et al. (2021) combined pipeline, tensor and data parallelism with a new pipeline scheduling strategy and named their approach PTD-P. Instead of only positioning a continuous set of layers (“model chunk”) on a device, each worker can be assigned with multiple chunks of smaller continuous subsets of layers (e.g. device 1 has layers 1, 2, 9, 10; device 2 has layers 3, 4, 11, 12; each has two model chunks). The number of microbatches in one batch should be exactly divided by the number of workers ($m % d = 0$). If there are $v$ model chunks per worker, the pipeline bubble time can be reduced by a multiplier of $v$ compared to a GPipe scheduling.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 7. Illustration of pipeline scheduling in PipeDream-2BW. (Image source: (Narayanan et al. 2021)\\nTensor Parallelism#\\nBoth model and pipeline parallelisms split a model vertically. OTOH we can horizontally partition the computation for one tensor operation across multiple devices, named Tensor parallelism (TP).\\nLet’s take the transformer as an example given its popularity. The transformer model mainly consists of layers of MLP and self-attention blocks. Megatron-LM (Shoeybi et al. 2020) adopts a simple way to parallelize intra-layer computation for MLP and self-attention.\\nA MLP layer in a transformer contains a GEMM (General matrix multiply) followed by an non-linear GeLU transfer. Let’s split weight matrix $A$ by column:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\text{Split }A &= [A_1, A_2] \\\\\\\\\\nY &=\\\\text{GeLU}(XA) \\\\\\\\\\n[Y_1, Y_2] &= [\\\\text{GeLU}(XA_1), \\\\text{GeLU}(XA_2)]\\n\\\\end{aligned}\\n$$'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='$$\\nL_\\\\text{aux} = w_\\\\text{aux} \\\\cdot \\\\text{CV}(\\\\sum_{x \\\\in X} G(x))^2\\n$$\\n\\nwhere $ \\\\text{CV}$ is the coefficient of variation and the loss weight $w_\\\\text{aux}$ is a hyperparameter to tune.\\nBecause every expert network only gets a fraction of training samples (“The shrinking batch problem”), we should try to use a batch size as large as possible in MoE. However, it is restricted by GPU memory. Data parallelism and model parallelism can be applied to improve the throughput.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Other Memory Saving Designs#\\nCPU Offloading#\\nWhen the GPU memory is full, one option is to offload temporarily unused data to CPU and read them back when needed later (Rhu et al. 2016). The idea of CPU offloading is straightforward but is less popular in recent years due to the slowdown it brings into the training time.\\nActivation Recomputation#\\nActivation recomputation (also known as “activation checkpointing” or “gradient checkpointing”; Chen et al. 2016) is a smart yet simple idea to reduce memory footprint at the cost of computation time. It reduces the memory cost of training a $\\\\ell$ layer deep neural net to $O(\\\\sqrt{\\\\ell})$, which only additionally consumes an extra forward pass computation per batch.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 2. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: Huang et al. 2019)\\nPipeline Parallelism#\\nPipeline parallelism (PP) combines model parallelism with data parallelism to reduce inefficient time “bubbles’’. The main idea is to split one minibatch into multiple microbatches and enable each stage worker to process one microbatch simultaneously. Note that every microbatch needs two passes, one forward and one backward. Inter-worker communication only transfers activations (forward) and gradients (backward). How these passes are scheduled and how the gradients are aggregated vary in different approaches. The number of partitions (workers) is also known as pipeline depth.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 14. An illustration of various parallelism strategies on how (Top) model weights and (Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens.  (Image source: Fedus et al. 2021)\\nBoth GShard top-2 and Switch Transformer top-1 depend on token choice, where each token picks the best one or two experts to route through. They both adopt an auxiliary loss to encourage more balanced load allocation but it does not guarantee the best performance. Furthermore, the expert capacity limit may lead to wasted tokens as they would be discarded if an expert reaches its capacity limit.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Export Choice (EC) (Zhou et al. 2022) routing instead enables each expert to select the top-$k$ tokens. In this way, each expert naturally guarantees a fixed capacity and each token may be routed to multiple experts. EC can achieve perfect load balancing and is shown to improve training convergence by 2x.\\nGiven $e$ experts and an input matrix $X \\\\in \\\\mathbb{R}^{n \\\\times d}$, the token-to-expert affinity scores are computed by:\\n$$\\nS = \\\\text{softmax}(X \\\\cdot W_g), \\\\text{where } W_g \\\\in \\\\mathbb{R}^{d \\\\times e}, S \\\\in \\\\mathbb{R}^{n \\\\times e}\\n$$\\nA token-to-expert assignment is represented by three matrices, $I, G \\\\in \\\\mathbb{R}^{e\\\\times k}$ and $P \\\\in \\\\mathbb{R}^{e \\\\times k \\\\times n}$. $I[i,j]$ annotates which token is the $j$-th selection by the $i$-th expert. The gating matrix $G$ stores the routing weights of selected tokens. $P$ is the one-hot version of $I$, used to produce the input matrix ($P \\\\cdot X \\\\in \\\\mathbb{R}^{e \\\\times k \\\\times d}$) for the gated FFN layer.\\n$$'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Let’s say, we evenly divide an $\\\\ell$-layer network into $d$ partitions. Only activations at partition boundaries are saved and communicated between workers. Intermediate activations at intra-partition layers are still needed for computing gradients so they are recomputed during backward passes. With activation recomputation, the memory cost for training $M(\\\\ell)$ is:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='The switch transformer paper summarized different data and model parallelism strategies for training large models with a nice illustration:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 9. (Top) Default `1F1B` pipeline schedule as in PipeDream-flush. (Bottom) Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and second chunks are in light colors. (Image source: Narayanan et al. 202))\\nMixture-of-Experts (MoE)#\\nThe Mixture-of-Experts (MoE) approach attracts a lot of attention recently as researchers (mainly from Google) try to push the limit of model size. The core of the idea is ensembling learning: Combination of multiple weak learners gives you a strong learner!\\nWithin one deep neural network, ensembling can be implemented with a gating mechanism connecting multiple experts (Shazeer et al., 2017). The gating mechanism controls which subset of the network (e.g. which experts) should be activated to produce outputs. The paper named it “sparsely gated mixture-of-experts” (MoE) layer.\\nPrecisely one MoE layer contains'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='$$\\nG, I = \\\\text{top-k}(S^\\\\top, k)\\\\quad P = \\\\text{one-hot}(I)\\n$$\\nOne regularization that export choice routing explored is to limit the maximum number of experts per token.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content=\"$$\\n\\\\begin{aligned}\\n& \\\\max_A \\\\langle S^\\\\top, A\\\\rangle + \\\\lambda H(A) \\\\\\\\\\n\\\\text{s.t.} & \\n\\\\forall i: \\\\sum_{j'} A[i, j'] = k,\\\\quad\\n\\\\forall j: \\\\sum_{i'} A[i', j] \\\\leq b,\\\\quad\\n\\\\forall i,j: 0 \\\\leq A[i,j] \\\\leq 1\\n\\\\end{aligned}\\n$$\"),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='In GPipe (Huang et al. 2019) gradients from multiple microbatches are aggregated and applied synchronously at the end. The synchronous gradient descent guarantees learning consistency and efficiency irrespective of the number of workers. As shown in Fig. 3, bubbles still exist but are much smaller than what’s in Fig. 2. Given $m$ evenly split microbatches and $d$ partitions, assuming both forward and backward per microbatch take one unit of time, the fraction of bubble is:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 11. Test perplexity on 1-Billion-Word language modeling benchmark. (Left) The model capacity increases from left to right, containing 4, 32, 256, 256, 1024 and 4096 experts. (Right) Performance of the 4 billion parameters MoE model, the largest one in the left figure, under different computation budgets. (Image source: Shazeer et al., 2017)\\nGShard (Lepikhin et al., 2020) scales the MoE transformer model up to 600 billion parameters with sharding. The MoE transformer replaces every other feed forward layer with a MoE layer. The sharded MoE transformer only has the MoE layers sharded across multiple machines, while other layers are simply duplicated.\\nThere are several improved designs for the gating function $G$ in GShard:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Weng, Lilian. (Sep 2021). How to train really large models on many GPUs? Lil’Log. https://lilianweng.github.io/posts/2021-09-25-train-large/.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 6. Illustration of pipeline scheduling in PipeDream-flush. (Image source: (Narayanan et al. 2021)\\nPipeDream-2BW maintains only two versions of model weights, where “2BW” is short for “double-buffered weights”. It generates a new model version every $k$ microbatches and $k$ should be larger than the pipeline depth $d$, $k > d$. A newly updated model version cannot fully replace the old version immediately since some leftover backward passes still depend on the old version. In total only two versions need to be saved so the memory cost is much reduced.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='$$\\n1 - \\\\frac{2md}{(2m + 2(d-1))d} = \\\\frac{d-1}{m+d-1}\\n$$\\n\\nThe GPipe paper observed that the bubble overhead is almost negligible if the number of microbatches is more than 4x the number of partitions $m > 4d$ (when activation recomputation is applied).')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = get_unique_union(results_all) # 去重\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 故而整个检索chain如下\n",
    "retriever = (\n",
    "        prompts_generate_query\n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "        | (lambda x: x.split('\\n'))\n",
    "        | retriever.map()\n",
    "        | get_unique_union\n",
    "        | format_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Parallelism#\\nThe most naive way for Data parallelism (DP)  is to copy the same model weights into multiple workers and assign a fraction of data to each worker to be processed at the same time.\\nNaive DP cannot work well if the model size is larger than a single GPU node’s memory. Methods like GeePS (Cui et al. 2016) offload temporarily unused parameters back to CPU to work with limited GPU memory when the model is too big to fit into one machine. The data swapping transfer should happen at the backend and not interfere with training computation.\\nAt the end of each minibatch, workers need to synchronize gradients or weights to avoid staleness. There are two main synchronization approaches and both have clear pros & cons.\\n\\nThe switch transformer paper summarized different data and model parallelism strategies for training large models with a nice illustration:\\n\\nBulk synchronous parallels (BSP): Workers sync data at the end of every minibatch. It prevents model weights staleness and good learning efficiency but each machine has to halt and wait for others to send gradients.\\nAsynchronous  parallel (ASP): Every GPU worker processes the data asynchronously, no waiting or stalling. However, it can easily lead to stale weights being used and thus lower the statistical learning efficiency. Even though it increases the computation time, it may not speed up training time to convergence.\\n\\nSomewhere in the middle is to synchronize gradients globally once every $x$ iterations ($x > 1$). This feature is called “gradient accumulation” in Distribution Data Parallel (DDP) since Pytorch v1.5 (Li et al. 2021). Bucketing gradients avoid immediate AllReduce operations but instead buckets multiple gradients into one AllReduce to improve throughput. Computation and communication scheduling optimization can be made based on the computation graph.\\n\\nTraining Parallelism#\\nThe main bottleneck for training very large neural network models is the intense demand for a large amount of GPU memory, way above what can be hosted on an individual GPU machine. Besides the model weights (e.g. tens of billions of floating point numbers), it is usually even more expensive to store intermediate computation outputs such as gradients and optimizer states (e.g. momentums & variations in Adam). Additionally training a large model often pairs with a large training corpus and thus a single process may just take forever.\\nAs a result, parallelism is necessary. Parallelism can happen at different dimensions, including data, model architecture, and tensor operation.\\nData Parallelism#\\nThe most naive way for Data parallelism (DP)  is to copy the same model weights into multiple workers and assign a fraction of data to each worker to be processed at the same time.\\n\\nFig. 3. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: Huang et al. 2019)\\nGPipe achieves almost linear speedup in throughput with the number of devices, although it is not always guaranteed if the model parameters are not evenly distributed across workers.\\nPipeDream (Narayanan et al. 2019) schedules each worker to alternatively process the forward and backward passes (1F1B).\\nPipeDream names each model partition “stage” and each stage worker can have multiple replicas to run data parallelism. In this process, PipeDream uses a deterministic round-robin load balancing strategy to assign work among multiple replicas of stages to ensure that the forward and backward passes for the same minibatch happen on the same replica.\\n\\nFig. 1. Pseudo code for Pytorch DDP. (Image source: Li et al. 2021)\\nModel Parallelism#\\nModel parallelism (MP) aims to solve the case when the model weights cannot fit into a single node. The computation and model parameters are partitioned across multiple machines. Different from data parallelism where each worker hosts a full copy of the entire model, MP only allocates a fraction of model parameters on one worker and thus both the memory usage and the computation are reduced.\\nSince deep neural networks usually contain a stack of vertical layers, it feels straightforward to split a large model by layer, where a small consecutive set of layers are grouped into one partition on one worker. However, a naive implementation for running every data batch through multiple such workers with sequential dependency leads to big bubbles of waiting time and severe under-utilization of computation resources.\\n\\n$$\\n1 - \\\\frac{2md}{(2m + 2(d-1))d} = \\\\frac{d-1}{m+d-1}\\n$$\\n\\nThe GPipe paper observed that the bubble overhead is almost negligible if the number of microbatches is more than 4x the number of partitions $m > 4d$ (when activation recomputation is applied).'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is DP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Fusion\n",
    "![img\\RAG-Fusion.png](img/RAG-Fusion.png)\n",
    "\n",
    "当启用多个query进行询问时，受到llm输入窗口的限制，不可能将所有的相关文本都输入进去(即使在去重的情况下)，所以使用得分的方式进行选取\n",
    "$$\\text{scors} = \\sum \\frac{1}{rank+k}$$\n",
    "k是防止分母为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "query_template = PromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumps, loads\n",
    "def reciprocal_rank_fusion(results_all: list[str], k=60):\n",
    "    scores_all = {}\n",
    "    for results in results_all:\n",
    "            for rank, result in enumerate(results):\n",
    "                result = dumps(result)\n",
    "                if result not in scores_all:\n",
    "                    scores_all[result] = 0\n",
    "                scores_all[result] += 1/(rank+k)\n",
    "    ranked_results = [ (loads(result), score) \n",
    "        for result, score in sorted(scores_all.items(), key=lambda x: x[0], reverse=True)\n",
    "    ]\n",
    "    return ranked_results\n",
    "\n",
    "query_chain = query_template|llm|StrOutputParser()|(lambda x: x.split('\\n'))|retriever.map()|reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='where each entry $A[i,j]$ in $A \\\\in \\\\mathbb{R}^{e \\\\times n}$ marks whether the $i$-the expert selects the $j$-th token. Solving this is non-trivial. The paper used Dykstra’s algorithm that runs a sequence of multiple iterative computation steps. Capped expert choice results in a slight decrease in the fine-tuning performance in the experiments.\\nThe parameter $k$ is determined by $k=nc/e$, where $n$ is the total number of tokens in one batch and $c$ is a capacity factor indicating the average number of experts used by one token. The paper used $c=2$ in most experiments, but EC with $c=1$ still outperforms the top-1 token choice gating. Interestingly, $c=0.5$ only marginally hurts the training performance.\\nOne big drawback of EC is that it does not work when the batch size is too small, neither for auto-regressive text generation, because it needs to know the future tokens to do the top-$k$ selection.\\nOther Memory Saving Designs#\\nCPU Offloading#'),\n",
       "  0.06558258417063283),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Their system Gist incorporates two encoding schemes:\\nLayer-specific lossless encoding; focus on ReLU-Pool (“Binarize”) and ReLU-Conv (“Sparse storage and dense computation”) patterns.\\nAggressive lossy encoding; use delayed precision reduction (DPR). They observed that the first immediate use of feature maps should be kept at high precision but the second use can tolerate lower precision.\\nThe experiments showed that Gist can reduce the memory cost by 2x across 5 SOTA image classification DNNs, with an average of 1.8x with only 4% performance overhead.\\nMemory Efficient Optimizer#\\nOptimizers are eager for memory consumption. Take the popular Adam optimizer as an example, it internally needs to maintain momentums and variances, both at the same scale as gradients and model parameters. All out of a sudden, we need to save 4x the memory of model weights.\\nSeveral optimizers have been proposed to reduce the memory footprint.'),\n",
       "  0.016666666666666666),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='The switch transformer paper summarized different data and model parallelism strategies for training large models with a nice illustration:'),\n",
       "  0.01639344262295082),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Several optimizers have been proposed to reduce the memory footprint.\\nFor example, instead of storing the full momentums and variations as in Adam, Adafactor (Shazeer et al. 2018) only tracks the per-row and per-column sums of the moving averages and then estimates the second moments based on these sums. SM3 (Anil et al. 2019) describes a different adaptive optimization method, leading to largely reduced memory as well.\\nZeRO (Zero Redundancy Optimizer; Rajbhandari et al. 2019) optimizes the memory used for training large models based on the observation about two major memory consumption of large model training:'),\n",
       "  0.03200204813108039),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Other Memory Saving Designs#\\nCPU Offloading#\\nWhen the GPU memory is full, one option is to offload temporarily unused data to CPU and read them back when needed later (Rhu et al. 2016). The idea of CPU offloading is straightforward but is less popular in recent years due to the slowdown it brings into the training time.\\nActivation Recomputation#\\nActivation recomputation (also known as “activation checkpointing” or “gradient checkpointing”; Chen et al. 2016) is a smart yet simple idea to reduce memory footprint at the cost of computation time. It reduces the memory cost of training a $\\\\ell$ layer deep neural net to $O(\\\\sqrt{\\\\ell})$, which only additionally consumes an extra forward pass computation per batch.'),\n",
       "  0.016129032258064516),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='In GPipe (Huang et al. 2019) gradients from multiple microbatches are aggregated and applied synchronously at the end. The synchronous gradient descent guarantees learning consistency and efficiency irrespective of the number of workers. As shown in Fig. 3, bubbles still exist but are much smaller than what’s in Fig. 2. Given $m$ evenly split microbatches and $d$ partitions, assuming both forward and backward per microbatch take one unit of time, the fraction of bubble is:'),\n",
       "  0.015873015873015872),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 6. Illustration of pipeline scheduling in PipeDream-flush. (Image source: (Narayanan et al. 2021)\\nPipeDream-2BW maintains only two versions of model weights, where “2BW” is short for “double-buffered weights”. It generates a new model version every $k$ microbatches and $k$ should be larger than the pipeline depth $d$, $k > d$. A newly updated model version cannot fully replace the old version immediately since some leftover backward passes still depend on the old version. In total only two versions need to be saved so the memory cost is much reduced.'),\n",
       "  0.03252247488101534),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Data Parallelism#\\nThe most naive way for Data parallelism (DP)  is to copy the same model weights into multiple workers and assign a fraction of data to each worker to be processed at the same time.\\nNaive DP cannot work well if the model size is larger than a single GPU node’s memory. Methods like GeePS (Cui et al. 2016) offload temporarily unused parameters back to CPU to work with limited GPU memory when the model is too big to fit into one machine. The data swapping transfer should happen at the backend and not interfere with training computation.\\nAt the end of each minibatch, workers need to synchronize gradients or weights to avoid staleness. There are two main synchronization approaches and both have clear pros & cons.'),\n",
       "  0.06507936507936507)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = query_chain.invoke(\"What is DP?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition\n",
    "将所提的问题进行解耦分成几步，然后针对几个子问题进行解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Decomposition 生成子问题\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "chain = prompt_decomposition|llm|StrOutputParser()|(lambda x: x.split('\\n'))\n",
    "question = chain.invoke(\"How DP works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the key principles behind dynamic programming?',\n",
       " '2. Can you provide examples of problems that can be solved using dynamic programming?',\n",
       " '3. How does dynamic programming differ from other problem-solving techniques like greedy algorithms or divide and conquer?']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question  # 但是最大的问题是提出的问题错了 DP是Data Parallelism不是动态规划"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer recursively\n",
    "根据得到问题后，如何处理问题的答案分为两种  \n",
    "\n",
    "\n",
    "![img\\Answer recursively.png](img/Answer_recursively.png)\n",
    "\n",
    "\n",
    "一种是进行迭代 获得结果后将结果送入下一个问题中促进回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "def format_qa_pair(question, answer):\n",
    "    formatted = \"\"\n",
    "    formatted = f\"Question:{question}\\nAnswer:{answer}\\n\\n\"\n",
    "    return formatted.strip()  # strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "q_a_pairs = \"\"\n",
    "# 通过prompt循环调用\n",
    "for q in question:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\")|retriever,\n",
    "         'question': itemgetter(\"question\"),\n",
    "         'q_a_pairs': itemgetter(\"q_a_pairs\")}\n",
    "         | decomposition_prompt\n",
    "         | llm\n",
    "         | StrOutputParser()\n",
    "    )\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs +  '\\n---\\n' + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dynamic programming differs from other problem-solving techniques like greedy algorithms and divide and conquer in several key ways:\\n\\n1. **Optimality**: Dynamic programming guarantees finding the optimal solution to a problem by breaking it down into subproblems and storing the results of these subproblems for future use. Greedy algorithms, on the other hand, make decisions based on the current best choice without considering the overall optimal solution. Divide and conquer also breaks down the problem into smaller subproblems but may not always guarantee the optimal solution.\\n\\n2. **Overlapping subproblems**: Dynamic programming identifies and solves overlapping subproblems only once, storing their solutions for future use. Greedy algorithms and divide and conquer may not efficiently handle overlapping subproblems, leading to redundant calculations and potentially slower performance.\\n\\n3. **Memoization**: Dynamic programming uses memoization to store the results of subproblems and avoid redundant calculations. Greedy algorithms and divide and conquer may not utilize memoization, leading to repeated computations and potentially slower execution.\\n\\n4. **Bottom-up approach**: Dynamic programming typically uses a bottom-up approach, starting from simpler subproblems and building up to the larger problem. Greedy algorithms and divide and conquer may not follow a strict bottom-up approach in their problem-solving strategies.\\n\\nOverall, dynamic programming stands out for its focus on optimality, efficient handling of overlapping subproblems, effective use of memoization, and systematic bottom-up approach compared to other problem-solving techniques like greedy algorithms and divide and conquer.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer individually \n",
    "\n",
    "![img\\Answer_individually.png](img/Answer_individually.png)\n",
    "\n",
    "\n",
    "单独问每个问题，结合所有的回答最终进行总回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\langchain\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "sub_q_generator = prompt_decomposition|llm|StrOutputParser()|(lambda x: x.split('\\n'))\n",
    "def retrieve_and_rag(question, prompt_rag, sub_q_generator):\n",
    "    question_all = sub_q_generator.invoke(question)\n",
    "    rag_r = []\n",
    "    for q in question_all:\n",
    "        retrieval_answer = retriever.get_relevant_documents(q)\n",
    "        answer = (prompt|llm|StrOutputParser()).invoke({\"question\": q, \"context\": retrieval_answer})\n",
    "        rag_r.append(answer)\n",
    "    return rag_r, question_all\n",
    "\n",
    "rag_r, question_all = retrieve_and_rag(\"How Data parallelism works?\", decomposition_prompt, sub_q_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The different types of data parallelism include Bulk Synchronous Parallelism (BSP) and Asynchronous Parallel (ASP). BSP involves syncing data at the end of every minibatch to prevent model weights staleness, while ASP processes data asynchronously without waiting, potentially leading to stale weights. Another approach is to synchronize gradients globally once every x iterations, known as \"gradient accumulation\" in Distribution Data Parallel (DDP).',\n",
       "  'Data parallelism improves performance in parallel computing by copying the same model weights into multiple workers and assigning a fraction of data to each worker to be processed simultaneously. This approach helps distribute the workload efficiently across multiple workers, increasing overall processing speed. Synchronization of gradients or weights at the end of each minibatch is essential to avoid staleness and ensure effective training.',\n",
       "  'Some common programming models used for implementing data parallelism include Data Parallelism, Model Parallelism, and Pipeline Parallelism. Data Parallelism involves copying the same model weights into multiple workers and assigning a fraction of data to each worker to be processed simultaneously. Model Parallelism partitions computation and model parameters across multiple machines to reduce memory usage and computation. Pipeline Parallelism combines model and data parallelism to reduce inefficient time \"bubbles\" by splitting one minibatch into multiple microbatches for simultaneous processing.'],\n",
       " ['1. What are the different types of data parallelism?',\n",
       "  '2. How does data parallelism improve performance in parallel computing?',\n",
       "  '3. What are some common programming models used for implementing data parallelism?'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_r, question_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data parallelism works by distributing the same model weights to multiple workers and assigning a portion of the data to each worker for simultaneous processing. This approach helps improve performance in parallel computing by dividing the workload efficiently across multiple workers, increasing overall processing speed. Synchronization of gradients or weights at the end of each minibatch is crucial to prevent staleness and ensure effective training. Common programming models used for implementing data parallelism include Bulk Synchronous Parallelism (BSP), Asynchronous Parallel (ASP), Data Parallelism, Model Parallelism, and Pipeline Parallelism. Each of these models offers different strategies for distributing workloads and optimizing performance in parallel computing environments.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    formatted = \"\"\n",
    "    for index, (question, answer) in enumerate(zip(question, answer)):\n",
    "        formatted += f\"Question{index}:{question}\\nAnswer:{answer}\\n\\n\"\n",
    "    return formatted.strip()\n",
    "\n",
    "context = format_qa_pair(question_all, rag_r)\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":\"How Data parallelism works?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Back\n",
    "\n",
    "问题太过于细节 通过将问题step—back 问一些原始的问题，检索到宽泛的上下文   \n",
    "有点像纯track了 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE\n",
    "\n",
    "![img\\HyDE.png](img/HyDE.png)\n",
    "\n",
    "单纯的问题和文本具有gap 故而让llm根据问题生成一部分的上下文进行增强检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    ")\n",
    "prompt_hyde = context_prompt|llm|StrOutputParser()|retriever\n",
    "results_retrieval = prompt_hyde.invoke(\"What is Data parallelism (DP)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Data Parallelism#\\nThe most naive way for Data parallelism (DP)  is to copy the same model weights into multiple workers and assign a fraction of data to each worker to be processed at the same time.\\nNaive DP cannot work well if the model size is larger than a single GPU node’s memory. Methods like GeePS (Cui et al. 2016) offload temporarily unused parameters back to CPU to work with limited GPU memory when the model is too big to fit into one machine. The data swapping transfer should happen at the backend and not interfere with training computation.\\nAt the end of each minibatch, workers need to synchronize gradients or weights to avoid staleness. There are two main synchronization approaches and both have clear pros & cons.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Training Parallelism#\\nThe main bottleneck for training very large neural network models is the intense demand for a large amount of GPU memory, way above what can be hosted on an individual GPU machine. Besides the model weights (e.g. tens of billions of floating point numbers), it is usually even more expensive to store intermediate computation outputs such as gradients and optimizer states (e.g. momentums & variations in Adam). Additionally training a large model often pairs with a large training corpus and thus a single process may just take forever.\\nAs a result, parallelism is necessary. Parallelism can happen at different dimensions, including data, model architecture, and tensor operation.\\nData Parallelism#\\nThe most naive way for Data parallelism (DP)  is to copy the same model weights into multiple workers and assign a fraction of data to each worker to be processed at the same time.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 1. Pseudo code for Pytorch DDP. (Image source: Li et al. 2021)\\nModel Parallelism#\\nModel parallelism (MP) aims to solve the case when the model weights cannot fit into a single node. The computation and model parameters are partitioned across multiple machines. Different from data parallelism where each worker hosts a full copy of the entire model, MP only allocates a fraction of model parameters on one worker and thus both the memory usage and the computation are reduced.\\nSince deep neural networks usually contain a stack of vertical layers, it feels straightforward to split a large model by layer, where a small consecutive set of layers are grouped into one partition on one worker. However, a naive implementation for running every data batch through multiple such workers with sequential dependency leads to big bubbles of waiting time and severe under-utilization of computation resources.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2021-09-25-train-large/'}, page_content='Fig. 2. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: Huang et al. 2019)\\nPipeline Parallelism#\\nPipeline parallelism (PP) combines model parallelism with data parallelism to reduce inefficient time “bubbles’’. The main idea is to split one minibatch into multiple microbatches and enable each stage worker to process one microbatch simultaneously. Note that every microbatch needs two passes, one forward and one backward. Inter-worker communication only transfers activations (forward) and gradients (backward). How these passes are scheduled and how the gradients are aggregated vary in different approaches. The number of partitions (workers) is also known as pipeline depth.')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data parallelism (DP) is a method where the same model weights are copied into multiple workers, and each worker is assigned a fraction of data to be processed simultaneously.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    ")\n",
    "answer_chain = (prompt|llm|StrOutputParser()).invoke({\"context\": results_retrieval, \"question\": \"What is Data parallelism (DP)?\"})\n",
    "answer_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing\n",
    "在有多个数据库的情况下，将分解的问题分配到不同的源数据库中，提高检索的效率   \n",
    "Given a question about LangChain usage, we'd want to infer which language the the question was referring to and query the appropriate docs. Query routing is the process of classifying which index or subset of indexes a query should be performed on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical and Semantic routing\n",
    "![img\\semantic_routing.png](img/semantic_routing.png)\n",
    "\n",
    "使用输出解析器，让LLM输出为固定的种类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal \n",
    "# Literal 是 Python 中的 typing 模块中的一个类型注解，用来定义一个函数参数、变量或者返回值只能取一组固定的字面值（字面量）之一。\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource\"\"\"\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ..., \n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",)\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system),\n",
    "     (\"human\", \"{question}\")]\n",
    ")\n",
    "\n",
    "chain = prompt|structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM的结构化输出\n",
    "\n",
    "![img\\structed_output.png](img/structed_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RouteQuery(datasource='python_docs'), 'python_docs')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "results = chain.invoke(question)\n",
    "results, results.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于一些函数想要连接到链中 使用RunnableLambda \n",
    "# lambda本身就可以连接到链中\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def choose_route(results):\n",
    "    if \"python_docs\" in results.datasource.lower():\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in results.datasource.lower():\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"golang_docs\" in results.datasource.lower():\n",
    "        return \"chain for python_docs\"\n",
    "\n",
    "chain_all = chain|RunnableLambda(choose_route)\n",
    "chain_all.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semantic routing\n",
    "当有多个prompt模板时候，可以使用embedding model编码prompt，让question选择最近似的\n",
    "\n",
    "![img\\semantic_routing2.png](img/semantic_routing2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.utils.math import cosine_similarity\n",
    "\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "prompt_template = [physics_template, math_template]\n",
    "prompt_embeddings = embedding.embed_documents(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 2, 1536)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt_embeddings),len(prompt_embeddings), len(prompt_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7529026 , 0.71731764]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\"query\": \"What's a black hole\"}\n",
    "query_embedding = embedding.embed_query(input[\"query\"])\n",
    "# cosine_similarity需要参数为List[List[float]] 传入的是一个[List[List[float]]\n",
    "similarity = cosine_similarity([query_embedding], prompt_embeddings) # 返回[List[List[float]]\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_router(input):\n",
    "    query_embedding = embedding.embed_query(input[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)\n",
    "    most_similar = prompt_template[similarity.argmax()]\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. This happens when a massive star collapses in on itself, creating a singularity with infinite density at its center. The boundary surrounding the singularity is called the event horizon, beyond which nothing can escape. Black holes can come in different sizes, from stellar-mass black holes to supermassive black holes found at the centers of galaxies.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "chain = ( \n",
    "        {\"query\": RunnablePassthrough()}\n",
    "        | RunnableLambda(prompt_router)\n",
    "        | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "chain.invoke(\"What's a black hole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Construaction\n",
    "\n",
    "需要vectorstores包含metadata\n",
    "通过源数据可以进行筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pbAd8O1Lvm4',\n",
       " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 21844,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
       " 'publish_date': '2024-02-07 00:00:00',\n",
       " 'length': 1058,\n",
       " 'author': 'LangChain'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "docs = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\", add_video_info=True\n",
    ").load()\n",
    "\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import datetime\n",
    "# 构建一个BaseModel 给一些属性进行检索\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch\n",
      "title_search: rag\n",
      "min_length_sec: 0\n",
      "max_length_sec: 600\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "将相关文档存储到Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-representation Indexing\n",
    "\n",
    "![img\\multi_rep_indexing.png](img/multi_rep_indexing.png)  \n",
    "\n",
    "\n",
    "有多个文档，虽然可以textsplitter 但是可以使用GPT-4进行summary后embedding,当近似度最大时，选取对应整个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'title': \"Thinking about High-Quality Human Data | Lil'Log\", 'description': '[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper “Vox populi”) and nice feedback. 🙏 ]\\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nThinking about High-Quality Human Data | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Thinking about High-Quality Human Data\\n    \\nDate: February 5, 2024  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nHuman Raters ↔ Data Quality\\n\\nThe Wisdom of the Crowd\\n\\nRater Agreement\\n\\nRater Disagreement & Two Paradigms\\n\\n\\nData Quality ↔ Model Training\\n\\nInfluence Functions\\n\\nPrediction Changes during Training\\n\\nNoisy Cross-Validation\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\n[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper “Vox populi”) and nice feedback. 🙏 ]\\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution. The community knows the value of high quality data, but somehow we have this subtle impression that “Everyone wants to do the model work, not the data work” (Sambasivan et al. 2021).\\n\\nFig. 1. Two directions to approach high data quality.\\nHuman Raters ↔ Data Quality#\\nCollecting human data involve a set of operation steps and every step contributes to the data quality:\\n\\nTask design: Design task workflow to improve clarity and reduce complexity. Detailed guidelines are helpful but very long and complicated guidelines demand a decent amount of training to be useful.\\nSelect and train a pool of raters: Select annotators with matched skillset and consistency. Training sessions are necessary. After onboarding, regular feedback and calibration sessions are also needed.\\nCollect and aggregate data. This is the stage where more ML techniques can be applied to clean, filter and smartly aggregate data to identify the true labels.\\n\\n\\nFig. 2. Quality assurance refers to a set of actions that allow one to improve quality by acting on the quality attributes identified in the quality model. (Image source: Daniel et al. 2018)\\nThe Wisdom of the Crowd#\\nVox populi (originally “Vox populi, vox Dei”), a Latin phrase, means the voice of people. A short paper named was the same name was published in 1907 on Nature. It tracked an event at an annual exhibition where a fat ox was selected and people would guess the weight of the ox in order to win a prize if the guess is close to the real number. The middlemost estimate was treated as “the vox populi” and ended up being very close to the true value. The author concluded “This result is, I think, more creditable to the trustworthiness of a democratic judgment than might have been expected.” This is probably the earliest mention of how crowdsourcing (“the wisdom of the crowd”) would work out.\\nAlmost 100 years later, Callison-Burch (2009) did an early study on using Amazon Mechanical Turk (AMT) to run non-expert human evaluation on Machine Translation (MT) tasks and even to rely on non-experts to create new gold reference translations. The setup for human evaluation was simple: Each turker is shown a source sentence, a reference translation, and 5 translations from 5 MT systems. They are asked to rank 5 translations from best to worst. Each task is completed by 5 turkers.\\nUnsurprisingly, there are spammers producing low quality annotation to only optimize the volume. So when measuring the agreement between experts and non-experts, different weighting schemes need to be applied to downweight the contribution of spammers: (1) “weighted by experts”: using agreement rate with experts on a gold set of 10 examples; (2) “weighted by non-experts”: relying on agreement rate with the rest of turkers on the whole dataset.\\nIn a harder task, non-expert human annotators were asked to create new gold reference translations. Callison-Burch designed the task in two stages, where the first stage created new translations with reference to MT outputs and the second one filtered translations that may seem to be gerated by a MT system. The correlation between experts’ and crowdsourced translations is higher than that between expert and MT system outputs.\\n\\nFig. 3. (Left) The agreement rate is measured by comparing each pair of translation sentences (\"A > B\", \"A=B\", \"A < B\") and thus chance agreement is 1/3. The upper bound is set by the expert-expert agreement rate. (Right) Comparison of BLEU score between translations from different sources. LCD (Linguistic Data Consortium) translators provide expert translations. (Image source: Callison-Burch 2009)\\nRater Agreement#\\nWe often think of annotation as targeting a single ground truth and try to evaluate quality against one gold answer with consistent standards. A common practice for finding reliable ground truth labels is to collect multiple labels from multiple raters. Assuming that each rater performs at a different level of quality, we can use a weighted average of annotations but weighted by a proficiency score. This score is often approximated by how often one rater agrees with others.\\nMajority Voting: Taking the majority vote is the simplest way of aggregation, equivalent to taking the mode of a set of labels. In this setting, every annotator is contributing equally.\\nRaw agreement (Tratz & Hovy, 2010): Raw agreement counts the percentage of other people agreeing with them. This is indirectly correlated to majority vote, because all members of the majority class are expected to get higher inter-annotator agreement rate.\\nCohen’s Kappa (Landis & Koch, 1977): Cohen’s kappa measures the inter-rater agreement in the form of $\\\\kappa = (p_o - p_e) / (1 - p_c)$, where $p_o$ is the raw agreement rate and $p_e$ is the agreement by chance. Cohen’s kappa has a correction term for agreeing by chance, but this correction may be overestimated if one label is more prevalent.\\nProbabilistic Graph Modeling: There is a body of work relying on probabilistic graph modeling to model different factors within annotation decisions, e.g. difficulty of the task, task latent topics, rater bias, rater confidence, and then predict the true labels accordingly. Zheng et al. (2017)  compared 17 algorithms on truth inference in crowdsourcing and most of them are probabilistic graph models.\\n\\nMACE (Multi-Annotator Competence Estimation; Hovy et al. 2013) is an early example of using graph modeling to estimate the likelihood of someone acting like a “spammer” by providing random labels. Unsurprisingly in cases when the incentive is misaligned, some annotators may behave as “spammers” to optimize the volume of tasks completed for higher pay. The goal of MACE is to identify spammers. Given a task $i$ and an annotator $j$, $T_i$ is the true label, $A_{ij}$ is the assigned label and $S_{ij}$ models the probability of annotator $j$ spamming. Then the generative process can be represented as belows. The parameter $\\\\theta_j$ defines the trustworthiness of the annotator $j$ (probability of not spamming) and the parameter $\\\\xi_j$  defines how an annotator behaves when they are spamming.\\n\\n\\n$$\\n\\\\begin{align}\\n& \\\\text{for } i = 1 \\\\dots N : \\\\\\\\\\n& \\\\quad T_i \\\\sim \\\\text{Uniform} \\\\\\\\\\n& \\\\quad \\\\text{for } j = 1 \\\\dots M : \\\\\\\\\\n& \\\\quad \\\\quad S_{ij} \\\\sim \\\\text{Bernoulli}(1 - \\\\theta_j) \\\\\\\\\\n& \\\\quad \\\\quad \\\\text{if } S_{ij} = 0 : \\\\\\\\\\n& \\\\quad \\\\quad \\\\quad A_{ij} = T_i \\\\\\\\\\n& \\\\quad \\\\quad \\\\text{else } : \\\\\\\\\\n& \\\\quad \\\\quad \\\\quad A_{ij} \\\\sim \\\\text{Multinomial}(\\\\xi_j) \\\\\\\\\\n\\\\end{align}\\n$$\\n\\nThen we can learn $\\\\theta, \\\\xi$ to maximize the observed data, in the form of the marginal data likelihood, where $A$ is the matrix of annotations, $S$ is the matrix of competence indicators and $T$ is the matrix of true labels:\\n\\n$$\\nP(A; \\\\theta, \\\\xi) = \\\\sum_{T, S} \\\\big[ \\\\prod_{i=1}^N P(T_i) \\\\cdot \\\\prod_{j=1}^M P(S_{ij}; \\\\theta_j) \\\\cdot P(A_{ij} \\\\vert S_{ij}, T_i; \\\\xi_j) \\\\big]\\n$$\\n\\nEither EM (Expectation–maximization) or VB (Variational Bayes) can be applied to maximize the above marginal likelihood. During EM optimization, at M-step, a fixed value $\\\\delta$ is added to the fractional counts before normalizing. During VB training, they applied symmetric Beta priors on $\\\\theta_j$  and symmetric Dirichlet priors on $\\\\xi_j$. When recovering the correct answers, we can take majority vote weighted by the annotators’ $\\\\theta$ estimates.\\nRater Disagreement & Two Paradigms#\\nThe aggregation process described above depends on an assumption that there exists one underlying gold answer and thus we can evaluate annotators’ performance accordingly. However, in many topics, especially in safety, social, or cultural areas, people can disagree and often this disagreement is valid and then it comes down to how much we want to apply a strict rule versus embracing diversity.\\nAroyo & Welty (2015) discussed a set of “myths” in the practice of human annotation collection and found all of them somewhat inaccurate, key findings including:\\n\\nOften there is more than one correct interpretation for some samples. We need diverse perspectives via e.g. having multiple people to review annotation quality.\\nDisagreement is not always bad. We should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information.\\n\\nIf it is caused by a task not well defined, we should enhance the instruction. However, a more detailed guideline does not resolve innate diversity among opinions.\\n\\n\\nExperts may not always be better than lay people, but they would have a big gap in terms of considering what’s important.\\nGround truth annotations can change in time, especially those related to timely events or news.\\n\\nLater, Rottger et al. (2021) formulated the difference into two contrasting paradigms for data annotation for subjective NLP tasks.\\n\\n\\n\\n\\nDescriptive\\nPrescriptive\\n\\n\\n\\n\\nDefinition\\nEncourage annotator subjectivity, trying to model many beliefs.\\nDiscourage annotator subjectivity, trying to consistently apply one belief.\\n\\n\\nPros\\n- Can help to identify which entries are more subjective;- Embrace diversity\\n- More aligned with standard NLP setup. - Easier to do QC by measuring disagreement or doing label aggregation.\\n\\n\\nCons\\n- Metrics like rater disagreement cannot be used to measure data quality or annotator performance; - Cannot be used for training models that are optimized for outputting one preset behavior.\\n- Expensive and challenging to create high-quality annotation guidelines, which can never be perfect, in practice;- Training annotators to get familiar with guideline in order to apply it properly is also challenging;- Cannot capture an interpretable diversity of beliefs or consistently encode one specific belief.\\n\\n\\n\\nThe descriptive paradigm allows us to understand a number of important effects as well as to account for different perspectives. For example, annotator identity (e.g. African American, LGBTQ) is found to be a statistically significant factor in how they would label identify-related content as toxic (Goyal et al. 2022). Topics can be another main driver for diverse opinions. Wang et al. (2023) studied the human evaluation process of safety of an AI conversation system and compared results between labels by Trust & Safety (T&S) professionals and crowdsourcing annotators. They intentionally collected rich metadata associated with crowd annotators like demographic or behavior information. Comparing T&S expert labels and crowd annotations, they found that agreement rates vary across semantic topics and the level of severity:\\n\\nAgreement rate differs a lot across different topics; ranging from 0.96 on violence/gory to 0.25 on personal topics.\\nAgreement rates are higher on “extreme” and “benign” conversations, given four label options marking “benign”, “debatable”, “moderate” to “extreme”.\\n\\n\\nFig. 4. Correlations between non-expert and expert annotations vary a lot across topics. (Image source: Wang et al. 2023)\\nZhang et al. (2023) proposed a taxonomy of rater disagreement to analyze the root causes. Among the listed causes, disagreement due to stochastic errors or inconsistency on the individual level should be avoided. In cases when a rater gives different labels to the same task when asked multiple times, some of those are most likely caused by human errors. Based on this intuition, the disagreement deconvolution method (Gordon et al. 2021) disentangles stable opinions from errors by anchoring each individual’s opinion to their own primary label and thus encouraging intra-rater consistency.\\n\\nFig. 5. A taxonomy of causes for rater disagreement. (Image source: Zhang et al. 2023)\\nDisagreement deconvolution relies on probabilistic graph modeling:\\n\\nEstimate how often an annotator returns non-primary labels, $p_\\\\text{flip}$\\nPer sample, get an adjusted label distribution $p^*$ of primary labels based on $p_\\\\text{flip}$\\nSample from $p^*$ as a new test set.\\nMeasure performance metrics against the new test set.\\n\\nGiven $C$-category classification, the sampling process of the generative model is stated as follows:\\n\\n$$\\n\\\\begin{aligned}\\ny^*\\\\mid x &\\\\sim \\\\text{Categorial}([C], p^*(y\\\\mid x)) \\\\\\\\\\ny_\\\\text{other}\\\\mid y^* &\\\\sim \\\\text{Categorial}([C]\\\\setminus\\\\{y^*\\\\}, \\\\frac{1}{C-1}) \\\\\\\\\\nz_\\\\text{flip} \\\\mid x &\\\\sim \\\\text{Bernoulli}(p_\\\\text{flip}(x)) \\\\\\\\\\ny\\\\mid y^*, y_\\\\text{other}, z_\\\\text{flip} &= y^* (1 - z_\\\\text{flip}) + y_\\\\text{other} z_\\\\text{flip}\\n\\\\end{aligned}\\n$$\\n\\nGiven the true $p(y\\\\mid x)$ and $p_\\\\text{flip}$ that can be estimated from the data, we would update the label distribution of primary labels:\\n\\n$$\\np^*(y\\\\mid x) = \\\\frac{p(y\\\\mid x) - \\\\frac{p_\\\\text{flip}(x)}{C-1}}{1 - \\\\frac{C \\\\cdot p_\\\\text{flip}(x)}{C - 1}}\\n$$\\n\\nA new test set sampled from $p^*(y \\\\mid x)$ represents the primary labels with individual inconsistency noise removed. It can be used for evaluation, as a noise-free test set.\\nTo capture systematic disagreement among annotators when learning to predict labels, Davani et al. (2021) experimented with a multi-annotator model where predicting each annotator’s labels is treated as one sub-task. Say, the classification task is defined on an annotated dataset $D=(X, A, Y)$, where $X$ is the text instances, $A$ is the set of annotators and $Y$ is the annotation matrix, $y_{ij} \\\\in Y$ represents a binary label assigned by $a_j \\\\in A$ to the sample $x_i \\\\in X$. The majority vote for $x_i$ is denoted as $\\\\bar{y}_{i,}$. The experiment is to train a classification head on top of a pre-trained BERT model and compares 4 setups:\\n\\nBaseline: Directly predict the majority vote $\\\\bar{y}_i$, not using the full annotation matrix $Y$.\\nEnsemble: Train one model per annotator separately to predict $y_{ij}$ and then the results are aggregated by majority vote.\\nMulti-label: Learn to predict $\\\\vert A \\\\vert$ labels to represent all annotators’ labels per sample $\\\\langle y_{i1}, \\\\dots, y_{i\\\\vert A \\\\vert} \\\\rangle$, with a shared MLP layer and then outputs are aggregated.\\nMulti-task: Similar to multi-label, but each annotator’s prediction head is learned from a separated MLP layer, such that we allocate extra compute to learn the difference among annotators.\\n\\nExperiment results on the GHC (Gab Hate Corpus) dataset showed that the multi-task model achieves the best F1 score and also can naturally provide prediction uncertainty estimation, correlated with annotation disagreement.\\n\\nFig. 6. Illustration of different architectures for modeling multiple annotators\\' labels. (Image source: Davani et al. 2021)\\nJury Learning (Gordon et al. 2022) mimics the jury process by modeling the different annotators’ labeling behavior conditioned on their characteristics. Starting with a dataset with labels and demographic characteristics of each labeler, we train a model to learn to predict labels made by every individual annotator, each as a potential juror. At decision time, practitioners can specify the composition of a group of jurors to determine a sampling strategy. The final decision is made by aggregating labels from jurors from multiple trials.\\n\\nFig. 7. Illustration of how jury learning works. (Image source: Gordon et al. 2022)\\nThe jury learning model is a DCN (Deep & Cross network) , commonly for recommendation use case,  that is jointly trained to learn comment embedding, annotator embedding and group (annotator’s characteristics) embedding. The text content is processed by a pre-trained BERT, which is also jointly fine-tuned but for a shorter period to avoid overfitting.\\n\\nFig. 8. DCN model architecture for jury learning. (Image source: Gordon et al. 2022)\\nTheir experiment runs on the toxicity diversity dataset and compares jury learning with a baseline model which is a fine-tuned BERT to predict individual annotator’s label without using metadata. Performance is measured in MAE (mean absolute error). Jury learning consistently outperforms the annotator-agnostic baseline on the full test set as well as each group segment.\\n\\nFig. 9. Experiment results comparing an annotator-agnostic baseline with jury learning. (Image source: Gordon et al. 2022)\\nData Quality ↔ Model Training#\\nOnce a dataset is constructed, many methods can help identify mislabels according to the training dynamics. Note that we only focus on methods to find and exclude data points with potentially incorrect labels, not about how to train a model with noisy data.\\nInfluence Functions#\\nInfluence functions is a classic technique from robust statistics (Hampel, 1974) to measure the effect of training data points by describing how the model parameters change as we upweight a training point by an infinitesimal amount. Koh & Liang (2017) introduced the concept to be applied to deep neural networks.\\nGiven $n$ data samples in the train set, $z_i = (x_i, y_i)$ for $i =1, \\\\dots, n$, The model parameter $\\\\theta$ is optimized to minimize a loss: $\\\\hat{\\\\theta} = \\\\arg\\\\min_{\\\\theta \\\\in \\\\Theta} \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\mathcal{L}(z_i, \\\\theta)$. The change of model parameters after we remove a single data point $z$ is denoted as $\\\\hat{\\\\theta}_{-z} - \\\\hat{\\\\theta}$ where $\\\\hat{\\\\theta}_{-z} = \\\\arg\\\\min_{\\\\theta \\\\in \\\\Theta} \\\\frac{1}{n} \\\\sum_{z_i \\\\neq z} \\\\mathcal{L}(z_i, \\\\theta)$. However, computing this literally for every sample is too expensive. One way to approximate this is to compute the parameter change given a small upweight $\\\\epsilon$ on $z$. By definition, the influence of upweighting $z$ by $\\\\epsilon$ is given by:\\n\\n$$\\n\\\\mathcal{I}_{\\\\text{up,params}}(z) = \\\\frac{d\\\\hat{\\\\theta}_{\\\\epsilon,z}}{d\\\\epsilon}\\\\bigg\\\\vert_{\\\\epsilon=0}=-\\\\mathbf{H}^{-1}_{\\\\hat{\\\\theta}} \\\\nabla_\\\\theta \\\\mathcal{L}(z, \\\\hat{\\\\theta})\\n$$\\n\\nwhere $\\\\hat{\\\\theta}_{\\\\epsilon,z} = \\\\arg\\\\min_{\\\\theta \\\\in \\\\Theta} \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\mathcal{L}(z_i, \\\\theta) + \\\\epsilon L(z, \\\\theta)$ and $\\\\mathbf{H}^{-1}_{\\\\hat{\\\\theta}} = \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\nabla^2_\\\\theta \\\\mathcal{L}(z_i, \\\\hat{\\\\theta})$.\\nRemoving a data point $x$ is equivalent to upweighting it by $\\\\epsilon = -\\\\frac{1}{n}$ and therefore $\\\\hat{\\\\theta}_{-z} - \\\\hat{\\\\theta} \\\\approx -\\\\frac{1}{n} \\\\mathcal{I}_{\\\\text{up,params}}(z)$.\\nThe influence of upweighting $z$ on the loss at a test point $z_\\\\text{test}$ is given by applying the chain rule:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathcal{I}_{\\\\text{up,loss}}(z, z_\\\\text{test}) \\n&= \\\\frac{d \\\\mathcal{L}(z_\\\\text{test}, \\\\hat{\\\\theta}_{\\\\epsilon,z})}{d\\\\epsilon}\\\\bigg\\\\vert_{\\\\epsilon=0} \\\\\\\\\\n&= \\\\nabla_\\\\theta \\\\mathcal{L}(z_\\\\text{test}, \\\\hat{\\\\theta})^\\\\top \\\\frac{d \\\\hat{\\\\theta}_{\\\\epsilon,z}}{d\\\\epsilon}\\\\bigg\\\\vert_{\\\\epsilon=0} \\\\\\\\\\n&= - \\\\nabla_\\\\theta \\\\mathcal{L}(z_\\\\text{test}, \\\\hat{\\\\theta})^\\\\top \\\\mathbf{H}^{-1}_{\\\\hat{\\\\theta}} \\\\nabla_\\\\theta \\\\mathcal{L}(z, \\\\hat{\\\\theta})\\n\\\\end{aligned}\\n$$\\n\\nUsing the influence function we can measure the effect of a single data point on model parameters and loss function in closed forms. It can help approximate leave-one-out retraining without actually running all the retraining. To identify mislabeled data, we can measure $\\\\mathcal{I}_\\\\text{up,loss}(z_i, z_i)$, approximating the prediction error on $z_i$ if $z_i$ is removed from the training set.\\n\\nFig. 10. Influence functions values match leave-one-out training results on 10-class MNIST. (Image source: Kohn & Liang, 2017)\\nGiven the closed form, influence functions is still hard to be scaled up because the inverse Hessian vector product is hard to compute. Grosse et al. (2023) experimented with the EK-FAC (Eigenvalue-corrected Kronecker-Factored Approximate Curvature; George et al. 2018) approximation instead.\\nPrediction Changes during Training#\\nAnother branch of methods are to track the changes of model prediction during training to identify cases which seem hard to be learned. Data Maps (Swayamdipta et al. 2020) tracks two attributes of model behavior dynamics during training to analyze the quality of dataset:\\n\\nConfidence: The model’s confidence in the true label, defined as the mean model probability of the true label across epochs. They also used a coarse-grained metric, “correctness”, defined as the fraction of times when the model predicts the correct label across epochs.\\nVariability: The variation of the confidence, defined as the standard deviation of model probability of the true label across epochs.\\n\\n\\nFig. 11. Data map for SNLI training set, based on a RoBERTa classifier. (Image source: Swayamdipta et al. 2020)\\nHard-to-learn (low confidence, low variability) samples are more likely to be mislabeled. They ran an experiment on WinoGrande dataset with 1% flipped label data. After retraining, flipped instances move to the lower confidence and slightly higher variability regions, indicating that the hard-to-learn regions contains mislabeled samples. Given this, we can train a classifier on equal numbers of label flipped and clean samples using only the confidence score (unsure why the paper didn’t use both confidence and variability as features). This simple noise classifier then can be used on the original dataset to identify potentially mislabeled instances.\\n\\nFig. 12. Data points originally with high confidence and low variability scores moved to low confidence, slightly higher variability regions after labels get flipped. (Image source: Swayamdipta et al. 2020)\\nHowever, we should not consider all hard-to-learn samples to be incorrect. In fact, the paper hypothesizes that ambiguous (high variability) and hard-to-learn (low confidence, low variability) samples are more informative for learning. Experiments showed that they are good for OOD generalization, giving better results on OOD eval, even in comparison to 100% training set.\\nTo investigate whether neural networks have a tendency to forget previously learned information, Mariya Toneva et al. (2019) designed an experiment: They track the model prediction for each sample during the training process and count the transitions for each sample from being classified correctly to incorrectly or vice-versa. Then samples can be categorized accordingly,\\n\\nForgettable (redundant) samples: If the class label changes across training epochs.\\nUnforgettable samples: If the class label assignment is consistent across training epochs. Those samples are never forgotten once learned.\\n\\nThey found that there are a large number of unforgettable examples that are never forgotten once learnt. Examples with noisy labels or images with “uncommon” features (visually complicated to classify) are among the most forgotten examples. The experiments empirically validated that unforgettable examples can be safely removed without compromising model performance.\\nIn the implementation, the forgetting event is only counted when a sample is included in the current training batch; that is, they compute forgetting across presentations of the same example in subsequent mini-batches. The number of forgetting events per sample is quite stable across different seeds and forgettable examples have a small tendency to be first-time learned later in the training. The forgetting events are also found to be transferable throughout the training period and between architectures.\\nPleiss, et al. (2020) developed a method named AUM (Area under the Margin) to spot wrong labels based on such an assumption: Say, a BIRD image is mistakenly marked as DOG. The gradient update would encourage generalization from other BIRD images to this BIRD image, while the DOG label provides an incorrect supervised signal to encourage the update to go another way. Hence, there exists tension between generalization and (wrong) prediction in gradient update signals.\\nGiven a classification dataset $(\\\\mathbf{x}, y) \\\\in \\\\mathcal{D}_\\\\text{train}$, let $z^{(t)}_i(\\\\mathbf{x}) \\\\in \\\\mathbb{R}$ be the logit corresponding to class $i$ at epoch $t$. The margin at epoch $t$ is the difference between the assigned logit and the next largest logit:\\n\\n$$\\nM^{(t)}(\\\\mathbf{x}, y) = z_y^{(t)}(\\\\mathbf{x}) - \\\\max_{i \\\\neq y} z^{(t)}_i(\\\\mathbf{x}),\\\\quad\\n\\\\text{AUM}(\\\\mathbf{x}, y) = \\\\frac{1}{T} \\\\sum^T_{t=1} M^{(t)}(\\\\mathbf{x}, y)\\n$$\\n\\nA negative margin indicates a wrong prediction and a large positive margin suggests high confidence in a correct prediction. The hypothesis is that mislabeled samples would have a smaller margin than correct samples due to the tension of generalization via SGD triggered by other samples.\\nIn order to determine the threshold, they insert fake data, named “threshold samples”, to determine the threshold:\\n\\nCreate a subset of threshold samples $\\\\mathcal{D}_\\\\text{thr}$.  If there are $N$ training samples for $C$ classes, we randomly sample $N/(C+1)$ samples and switch all their labels to a fake new class $C+1$.\\nMerge threshold samples into the original dataset: $\\\\mathcal{D}’ = { (\\\\mathbf{x}, C+1): \\\\mathbf{x} \\\\in \\\\mathcal{D}_\\\\text{thr}} \\\\cup (\\\\mathcal{D} \\\\setminus\\\\mathcal{D}_\\\\text{thr})$;\\nTrain the model on $\\\\mathcal{D}’$ and measure AUM of all the data;\\nCompute the threshold $\\\\alpha$ as the 99th percentile of AUM of threshold samples;\\nIdentify mislabeled data using $\\\\alpha$ a threshold: ${(\\\\mathbf{x}, y) \\\\in \\\\mathcal{D} \\\\setminus \\\\mathcal{D}_\\\\text{thr}: \\\\text{AUM}_{\\\\mathbf{x}, y} \\\\leq \\\\alpha}$\\n\\n\\nFig. 13. How the AUM of threshold samples help separate out mislabeled samples. (Image source: Pleiss et al. 2020)\\n\\nFig. 14. Test error on CIFAR 10/100 with randomly mislabeled samples, comparing different methods for data filter or noisy data training. (Image source: Pleiss et al. 2020)\\nNoisy Cross-Validation#\\nThe NCV (Noisy Cross-Validation) method (Chen et al. 2019) divides the dataset into half at random, and then identifies data samples as “clean” if its label matches the predicted label provided by the model that is only trained on the other half of the dataset. Clean samples are expected to be more trustworthy. INCV (Iterative Noisy Cross-Validation) runs NCV iteratively where more clean samples are added into the trusted candidate set $\\\\mathcal{C}$ and more noisy samples are removed.\\n\\nFig. 15. Algorithm of INCV (iterative noisy cross-validation). (Image source: Chen et al. 2019)\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Feb 2024). “Thinking about High-Quality Human Data”. Lil’Log. https://lilianweng.github.io/posts/2024-02-05-human-data-quality/.\\n\\nOr\\n@article{weng2024humandata,\\n  title   = \"Thinking about High-Quality Human Data\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Feb\",\\n  url     = \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\"\\n}\\nReferences#\\n[1] Francis Galton “Vox populi”  Nature 75, 450-451 (1907).\\n[2] Sambasivan et al. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\" CHI 2021\\n[3] Chris Callison-Burch. “Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk” EMNLP 2009\\n[4] Rottger et al. “Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks” NAACL 2022.\\n[5] Aroyo & Welty “Truth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation” AI Magazine\\xa036.1: 15-24 (2015).\\n[6] Hovy et al. “Learning Whom to Trust with MACE” NAACL-HLT 2013.\\n[7] Wang et al. “All that Agrees Is Not Gold: Evaluating Ground Truth Labels and Dialogue Content for Safety” 2023.\\n[8] Zhang et al. “A Taxonomy of Rater Disagreements: Surveying Challenges & Opportunities from the Perspective of Annotating Online Toxicity” arXiv preprint arXiv:2311.04345\\xa0(2023).\\n[9] Davani et al. “Dealing with disagreements: Looking beyond the majority vote in subjective annotations” ACL 2022.\\n[10] Gordon et al. “Jury Learning: Integrating Dissenting Voices into Machine Learning Models” CHI 2022.\\n[11] Gordon et al. “The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality” CHI 2021\\n[12] Daniel et al. 2018 “Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions” ACM Computing Surveys (CSUR), 51(1), 1-40 (2018).\\n[13] Koh & Liang. “Understanding Black-box Predictions via Influence Functions” ICML 2017.\\n[14] Grosse et al. “Studying Large Language Model Generalization with Influence Functions” arXiv preprint arXiv:2308.03296 (2023).\\n[15] Swayamdipta et al. “Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics” EMNLP 2020.\\n[16] Toneva, et al. “An Empirical Study of Example Forgetting during Deep Neural Network Learning” ICLR 2019.\\n[17] Pleiss, et al.  “Identifying Mislabeled Data using the Area Under the Margin Ranking” NeuriPS 2020.\\n[18] Chen et al. “Understanding and utilizing deep neural networks trained with noisy labels” ICML 2019.\\n\\n\\n\\ndata\\ndata-quality\\nhuman-ai\\n\\n\\n\\n« \\n\\nDiffusion Models for Video Generation\\n\\n\\n »\\n\\nAdversarial Attacks on LLMs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "        {\"doc\": lambda x: x.page_content}\n",
    "         | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "         | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "         | StrOutputParser()\n",
    "            ) \n",
    " # 'max_concurrency' for controlling how much work to do in parallel\n",
    "summaries = chain.batch(docs,{\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The document discusses the concept of building autonomous agents powered by Large Language Models (LLMs). It covers key components of such agents, including planning, memory, and tool use, along with case studies and proof-of-concept examples. Challenges such as finite context length, planning difficulties, and reliability of natural language interfaces are also highlighted. The document provides references to related research and projects in the field.',\n",
       " 'The document discusses the importance of high-quality human data for training deep learning models. It covers topics such as the role of human raters in data quality, the wisdom of the crowd in data annotation, methods for measuring rater agreement, and two paradigms for data annotation. It also explores techniques for identifying mislabeled data points during model training, such as influence functions, tracking prediction changes during training, and noisy cross-validation. The document provides citations to relevant studies and research in the field of data quality and model training.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.stores import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "id_key = \"doc_id\"\n",
    "doc_ids = [str(i) for i,_ in enumerate(docs)]\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "vector_store = FAISS.from_documents(summary_docs, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '1'}, page_content='The document discusses the importance of high-quality human data for training deep learning models. It covers topics such as the role of human raters in data quality, the wisdom of the crowd in data annotation, methods for measuring rater agreement, and two paradigms for data annotation. It also explores techniques for identifying mislabeled data points during model training, such as influence functions, tracking prediction changes during training, and noisy cross-validation. The document provides citations to relevant studies and research in the field of data quality and model training.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":1})\n",
    "retriever.get_relevant_documents(\"What is DP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPTOR\n",
    "\n",
    "\n",
    "![img\\RAPTOR.png](img/RAPTOR.png)  \n",
    "\n",
    "\n",
    "\n",
    "通过文本进行分层，有些问题需要详细的信息，有些问题需要广泛的信息\n",
    "\n",
    "code: https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb   \n",
    "video: https://www.youtube.com/watch?v=jbGchdTL7d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass # todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColBERT\n",
    "ColBERT generates a contextually influenced vector for each token in the passages. \n",
    "\n",
    "ColBERT similarly generates vectors for each token in the query.\n",
    "\n",
    "Then, the score of each document is the sum of the maximum similarity of each query embedding to any of the document embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass # todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG from Scratch: Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-ranking\n",
    "![img\\re-ranking.png](img/re-ranking.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "loader = WebBaseLoader(\n",
    "    web_path=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")),\n",
    "    ),\n",
    "    )\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "doc_splitted = text_splitter.split_documents(docs)\n",
    "vector_store = FAISS.from_documents(doc_splitted, embedding=OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\n",
    "    \"\"\"\n",
    ")\n",
    "generate_quers = (\n",
    "    template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在query_Transformers中根据出现的评论进行赋分, reciprocal_rank_fusion 就是一种rerank技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere Re-rank\n",
    "![img\\Cohere_ReRank.png](img/Cohere_ReRank.png)   \n",
    "Blog: https://cohere.com/blog/rerank  \n",
    "在Rerank阶段,LLMs计算一个分数，该分数为query喝每一个最初搜索结果的相似性得分\n",
    "> 感觉是就是一阶段通过语义或者是单词进行搜索，二阶段整体语言再rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\langchain\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_cohere import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "# ContextualCompressionRetriever \n",
    "# Retriever that wraps a base retriever and compresses the results.\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-english-v2.0\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever)\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\"What is task decomposition for LLM agents?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.9972744}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.95622724}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.94407386}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img\\self_rag.png](img/self_rag.png)   \n",
    "* 检索节点\n",
    "  * 从文档中检索除文档块\n",
    "  * 输出:yes no continue\n",
    "* Grade 评分\n",
    "  * 评估文档块中是否提供了解决问题的有用信息\n",
    "  * relevant irrelevant\n",
    "* 生成 \n",
    "  * 根据生成的内容是否和文档块相关 \n",
    "  * fully supporrted partially supported no support\n",
    "* 答案评估\n",
    "  * generation是否对question进行回复\n",
    "  * 1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "docs = [WebBaseLoader(url,bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")),\n",
    "    ),).load() for url in urls]\n",
    "docs_list = [sublist for doc in docs for sublist in doc ]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
    "doc_splitted = text_splitter.split_documents(docs_list)\n",
    "vector_store = FAISS.from_documents(doc_splitted, embedding=OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下构建图的不同部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# LLMs想要结构化输出使用BaseModel修饰的类\n",
    "class GradeDocumnet(BaseModel):\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "structured_llm = llm.with_structured_output(GradeDocumnet)\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader = grade_prompt | structured_llm\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_text = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"document\": doc_text, \"question\": question}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "generation = rag_chain.invoke({\"question\": \"agent memory\", \"context\": format_docs(docs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Agent memory refers to the long-term memory capability of retaining and recalling infinite information over extended periods. This is often achieved through an external vector store and fast retrieval. The memory stream module records a comprehensive list of the agent's experiences in natural language.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucination(binary_score='yes')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hallucination Grader 幻觉评分\n",
    "\n",
    "class GradeHallucination(BaseModel):\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucination)\n",
    "system = \"\"\"\n",
    "    you are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\n",
    "\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, int, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Answer Grader 结果评分\n",
    "# data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: int = Field(description=\"Score from 0 to 5, 5 being the best answer\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "structured_llm = llm.with_structured_output(GradeAnswer)\n",
    "system = \"\"\"\n",
    "    You are a grader assessing the quality of an answer generated by an LLM. \\n \n",
    "    Give a score from 0 to 5, 5 being the best answer.\n",
    "\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {answer}\"),\n",
    "    ]\n",
    ")\n",
    "answer_grader = answer_prompt | structured_llm\n",
    "grade = answer_grader.invoke({\"question\": question,\"answer\": generation})\n",
    "grade.binary_score, type(grade.binary_score), int(grade.binary_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the role of memory in an agent's functioning?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question Re-writer\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying sematic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "from typing import List\n",
    "# TypingDict 用于定义字典类型 让键值对的类型固定\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    对于图的表示\n",
    "    Attributes:\n",
    "        question: str: 问题\n",
    "        generation: str: 生成的答案\n",
    "        documents: list[Document]: 文档\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: list[Document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"检索相关文档\"\"\"\n",
    "    print(\"Retrieving documents...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": docs, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"生成回答\"\"\"\n",
    "    print(\"Generating answer...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"question\": question, \"context\": format_docs(docs)})\n",
    "    return {\"documents\": docs, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"判别检索的文档是否和问题相关\"\"\"\n",
    "    print(\"Grading documents...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        print(doc.page_content)\n",
    "        score = retrieval_grader.invoke({\"document\": doc.page_content, \"question\": question})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"Document relevant\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"Document not relevant\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"转换问题\"\"\"\n",
    "    print(\"Transforming query...\")\n",
    "    question = state[\"question\"]\n",
    "    new_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": state[\"documents\"], \"question\": new_question}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"判断是否生成答案\"\"\"\n",
    "    print(\"Deciding to generate...\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "    if not filtered_documents:\n",
    "        # 所有检索的文档都不好 重修将问题转换为新问题 再次检索\n",
    "        print(\"No relevant documents. Transforming query...\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"Generating answer...\")\n",
    "        return \"generate\"\n",
    "    \n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"评分生成的答案\"\"\"\n",
    "    print(\"Grading generation...\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        print(\"Grading answer...\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"answer\": generation})\n",
    "        if score.binary_score < 3:\n",
    "           print(f\"{score.binary_score} is a low score. generation does not address question...\")\n",
    "           return \"not useful\"\n",
    "        else:\n",
    "            print(f\"{score.binary_score} is a high score. generation is good...\")\n",
    "            return \"useful\"\n",
    "    else:\n",
    "        print(\"Generation not grounded in facts\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建图\n",
    "from langgraph.graph import StateGraph, END\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"transform_query\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {   \n",
    "        \"not supported\": \"transform_query\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents...\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "Grading documents...\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "Document relevant\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "Document not relevant\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Document not relevant\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "Document not relevant\n",
      "Deciding to generate...\n",
      "Generating answer...\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "Generating answer...\n",
      "Grading generation...\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "Grading answer...\n",
      "4 is a high score. generation is good...\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('Long-term memory allows agents to store and recall vast amounts of '\n",
      " 'information over extended periods by utilizing external storage and quick '\n",
      " 'retrieval mechanisms.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         +-----------+                    \n",
      "                         | __start__ |                    \n",
      "                         +-----------+                    \n",
      "                               *                          \n",
      "                               *                          \n",
      "                               *                          \n",
      "                         +----------+                     \n",
      "                         | retrieve |*                    \n",
      "                         +----------+ ****                \n",
      "                        **                ****            \n",
      "                      **                      ****        \n",
      "                    **                            ****    \n",
      "        +-----------------+                           *** \n",
      "        | grade_documents |                             * \n",
      "        +-----------------+                             * \n",
      "           ...          ..                              * \n",
      "          .               ..                            * \n",
      "        ..                  ..                          * \n",
      "+----------+                  ...                       * \n",
      "| generate |....                 ..                  ***  \n",
      "+----------+    ......             ..               *     \n",
      "           ...        .......        ..          ***      \n",
      "              .              ......    ..       *         \n",
      "               ..                  ....  ..   **          \n",
      "             +---------+           +-----------------+    \n",
      "             | __end__ |           | transform_query |    \n",
      "             +---------+           +-----------------+    \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentic_RAG\n",
    "Agent为使用工具   \n",
    "故而Agentic_RAG为 Retrieal Agent(an LLM access to a retriever tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "docs = [WebBaseLoader(url,bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")),\n",
    "    ),).load() for url in urls]\n",
    "docs_list = [sublist for doc in docs for sublist in doc]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50) # 切的太小，运行时间长\n",
    "doc_splitter = text_splitter.split_documents(docs_list)\n",
    "vector_store = FAISS.from_documents(doc_splitter, embedding=OpenAIEmbeddings())\n",
    "retreiver = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16132\\3224597768.py:12: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  tool_executor = ToolExecutor(tools)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个检索工具\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolExecutor # Executes a tool invocation.\n",
    "from langgraph.prebuilt import ToolNode\n",
    "tool = create_retriever_tool(\n",
    "    retreiver,  # retriever\n",
    "    \"retrieve_blog_posts\", # name\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\", # description\n",
    ")\n",
    "tools = [tool]\n",
    "ToolNode = ToolNode(tools)\n",
    "tool_executor = ToolExecutor(tools) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来构建一个langgraph\n",
    "\n",
    "![img\\RAG_agent.png](img/RAG_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个状态 我理解的状态其实就是想要保存到变量\n",
    "# 该状态一般继承了TypedDict\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "class GraphState(TypedDict):\n",
    "    # Annotated允许为类型提示添加元数据 即Seuqence[BaseMessage]类型的messages操作为add\n",
    "    # Sequence 代表了一个泛型的、有序的集合。\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import format_tool_to_openai_function\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "from langchain_core.messages import FunctionMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "# 构建判别节点\n",
    "def should_retrieve(stat):\n",
    "    \"\"\"判断是否需要检索来处理问题\"\"\"\n",
    "    print(\"--decide to retrieve--\")\n",
    "    messages = stat[\"messages\"]\n",
    "    last = messages[-1]\n",
    "    if \"function_call\" not in last.additional_kwargs:\n",
    "        print(\"--decision: do not retrieve--\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"--decision: retrieve--\")\n",
    "        return \"continue\"\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"对检索的文档进行评估 是否和问题有关\"\"\"\n",
    "    print(\"--check relevance--\")\n",
    "\n",
    "    class grade(BaseModel):\n",
    "        binary_score: str=Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "    grade_tool_llm = convert_to_openai_tool(grade)\n",
    "    llm_with_tool = model.bind(\n",
    "        tools = [grade_tool_llm],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}})\n",
    "    # 以上使用convert_to_openai_tool 等同于使用 with_structured_output\n",
    "    parser = PydanticToolsParser(tools=[grade]) # Parse tools from OpenAI response.\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool | parser\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "    score = chain.invoke({\"context\": docs, \"question\": question})\n",
    "    grade = score[0].binary_score\n",
    "    if grade == \"yes\":\n",
    "        print(\"Document relevant\")\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        print(\"Document not relevant\")\n",
    "        return \"no\"\n",
    "\n",
    "# 构建节点\n",
    "def agent(state):\n",
    "    \"\"\"基于当前的状态生成响应， 使用检索还是结束\"\"\"\n",
    "    print(\"--call agent--\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2, streaming=True)\n",
    "    functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "    model = model.bind_functions(functions)\n",
    "    responses = model.invoke(messages)\n",
    "    return {\"messages\": [responses]}\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"使用工具去检索\"\"\"\n",
    "    print(\"--executing retrieve--\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    action = ToolInvocation(\n",
    "        tool = last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input = \n",
    "        json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
    "    )\n",
    "    responese = tool_executor.invoke(action)\n",
    "#     message_with_single_tool_call = AIMessage(\n",
    "#     content=\"\",\n",
    "#     tool_calls=[\n",
    "#         {\n",
    "#             \"name\": last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "#             \"args\": last_message.additional_kwargs[\"function_call\"]['arguments'],\n",
    "#             \"id\": last_message['id'],\n",
    "#             \"type\": \"function_call\",\n",
    "#         }\n",
    "#     ],\n",
    "# )\n",
    "#     responese = ToolNode.invoke({\"messages\": [message_with_single_tool_call]})\n",
    "    function_message = FunctionMessage(content=str(responese), name=last_message.additional_kwargs[\"function_call\"][\"name\"])\n",
    "    return {\"messages\": [function_message]}\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    重写问题来产生更好的问题\n",
    "    \"\"\"\n",
    "    print(\"--transforming query--\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    msg = [HumanMessage(\n",
    "        content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "    )]\n",
    "    model = ChatOpenAI(temperature=0.2, model=\"gpt-3.5-turbo\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"生成答案\"\"\"\n",
    "    print(\"--generating answer--\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "    chat_template = PromptTemplate.from_template(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "    Question: {question} \n",
    "\n",
    "    Context: {context} \n",
    "\n",
    "    Answer:\"\"\",\n",
    "\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = (\n",
    "        chat_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    response = rag_chain.invoke({\"question\": question, \"context\": format_docs(docs)})\n",
    "    return {\"messages\": [response]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_retrieve,\n",
    "    {\n",
    "        \"continue\": \"retrieve\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,\n",
    "    {\n",
    "        \"yes\": \"generate\",\n",
    "        \"no\": \"rewrite\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--call agent--\n",
      "--decide to retrieve--\n",
      "--decision: retrieve--\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"types of agent memory\"}', 'name': 'retrieve_blog_posts'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-3eed99ec-8245-46c8-be1f-131c5debf2ce-0')]}\n",
      "'\\n---\\n'\n",
      "--executing retrieve--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16132\\3338763594.py:78: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--check relevance--\n",
      "Document not relevant\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ FunctionMessage(content='Types of Memory#\\n\\ninformation. There are several types of memory in human brains.\\n\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite)', name='retrieve_blog_posts')]}\n",
      "'\\n---\\n'\n",
      "--transforming query--\n",
      "\"Output from node 'rewrite':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='What are the different types of agent memory discussed by Lilian Weng?', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 62, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-97994b92-2eed-4789-a98f-71fd72fbce4b-0', usage_metadata={'input_tokens': 62, 'output_tokens': 15, 'total_tokens': 77})]}\n",
      "'\\n---\\n'\n",
      "--call agent--\n",
      "--decide to retrieve--\n",
      "--decision: do not retrieve--\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='Lilian Weng discusses the following types of agent memory in her blog post:\\n\\n1. Short-term memory\\n2. Long-term memory\\n3. GPT-3.5 powered Agents for delegation of simple tasks\\n\\nThese types of memory play a crucial role in the functioning of agents and their ability to retain and recall information.', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-01f7c6b5-9421-45f0-a334-bf6e1cc3b2ff-0')]}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\n",
    "    \"messages\":[\n",
    "        HumanMessage(content=\"What does Lilian Weng say about the types of agent memory?\")\n",
    "    ]\n",
    "}\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Output from node '{key}':\")\n",
    "        pprint(\"---\")\n",
    "        pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    +-----------+                \n",
      "                    | __start__ |                \n",
      "                    +-----------+                \n",
      "                           *                     \n",
      "                           *                     \n",
      "                           *                     \n",
      "                      +-------+                  \n",
      "                      | agent |.                 \n",
      "                   ...+-------+ ....             \n",
      "                ...        *        ....         \n",
      "            ....           *            ...      \n",
      "          ..               *               ....  \n",
      "+----------+               *                   ..\n",
      "| retrieve |               *                    .\n",
      "+----------+....           *                    .\n",
      "      .         ...        *                    .\n",
      "      .            ....    *                    .\n",
      "      .                ..  *                    .\n",
      "+----------+          +---------+              ..\n",
      "| generate |          | rewrite |          ....  \n",
      "+----------+****      +---------+       ...      \n",
      "                ***                 ....         \n",
      "                   ****         ....             \n",
      "                       **     ..                 \n",
      "                      +---------+                \n",
      "                      | __end__ |                \n",
      "                      +---------+                \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive RAG\n",
    "核心技术有两点: \n",
    "* query analysis： query的构建分析 https://blog.langchain.dev/query-construction/\n",
    "* self-corrective RAG: 对自我进行西征 所谓的修正 即使对context的管理 和回答的质量\n",
    "\n",
    "对于query analysis: 1. no retrieval 2. single-shot RAG 3. iterative RAG   \n",
    "对于route: 1. Web search 和实事相关 2. self-corrective RAG 和文档相关\n",
    "\n",
    "![img\\adaptive_RAG.png](img/adaptive_RAG.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\"\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed = OpenAIEmbeddings()\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "docs = [WebBaseLoader(url,bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")),)).load() \n",
    "        for url in urls]\n",
    "docs_list = [sublist for doc in docs for sublist in doc]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "doc = text_splitter.split_documents(docs_list)\n",
    "vector_store = FAISS.from_documents(doc, embedding=embed)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "route='web_search'\n",
      "route='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "### router 根据问题选择不同的处理方式\n",
    "\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"根据语义将问题分配给最相关的数据源\"\"\"\n",
    "    route: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "question_router = route_prompt | structured_llm_router\n",
    "print(question_router.invoke({\"question\": \"Who will the Bears draft first in the NFL draft?\"}))\n",
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader 检索评分\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents. \"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_text = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"document\": doc_text, \"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative agents combine LLM with memory, planning, and reflection mechanisms to behave based on past experience and interact with other agents. The memory stream records agents' experiences in natural language, with each element being an observation or event provided by the agent. The retrieval model surfaces context based on relevance, recency, and importance to inform the agent's behavior.\n"
     ]
    }
   ],
   "source": [
    "### 生成 根据检索生成\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "generation = rag_chain.invoke({\"question\": \"agent memory\", \"context\": format_docs(docs)})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucination(binary_score='yes')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader  幻觉评分 来根据相关文档评估回复是否满意\n",
    "class GradeHallucination(BaseModel):\n",
    "    \"\"\"Binary score for hallucination check on generated text. \"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "structured_llm = llm.with_structured_output(GradeHallucination)\n",
    "\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "hallucination_grader = hallucination_prompt | structured_llm\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, int, 4)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader 结果评分 对结果是否满意\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Score for grading the quality of an answer. \"\"\"\n",
    "    binary_score: int = Field(description=\"Score from 0 to 5, 5 being the best answer\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "structured_llm = llm.with_structured_output(GradeAnswer)\n",
    "system = \"\"\"You are a grader assessing the quality of an answer generated by an LLM. \\n \n",
    "    Give a score from 0 to 5, 5 being the best answer.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {answer}\"),\n",
    "    ]\n",
    ")\n",
    "answer_grader = answer_prompt | structured_llm\n",
    "grade = answer_grader.invoke({\"question\": question,\"answer\": generation})\n",
    "grade.binary_score, type(grade.binary_score), int(grade.binary_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Can you provide an overview of the different types of memory used by agents in artificial intelligence systems and discuss their importance in enhancing the performance of AI applications?\"'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer 问题重写 \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "sysytem = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying sematic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WebSearch Tool 网络搜索工具\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 构建图\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"State for the graph.\"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[Document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建节点\n",
    "# 图中的节点还是判断等都是函数，函数的输入为state,输出为state\n",
    "def retrieve(state):\n",
    "    \"\"\"检索相关文档\"\"\"\n",
    "    print(\"Retrieving documents...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": docs, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"生成回答\"\"\"\n",
    "    print(\"Generating answer...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"documents\"]\n",
    "    print(docs)\n",
    "    print(type(docs))\n",
    "    generation = rag_chain.invoke({\"question\": question, \"context\": format_docs(docs)})\n",
    "    return {\"documents\": docs, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"判别检索的文档是否和问题相关\"\"\"\n",
    "    print(\"Grading documents...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        print(doc.page_content)\n",
    "        score = retrieval_grader.invoke({\"document\": doc.page_content, \"question\": question})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"Document relevant\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"Document not relevant\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"转换问题,当没有相关文档时使用\"\"\"\n",
    "    print(\"Transforming query...\")\n",
    "    question = state[\"question\"]\n",
    "    new_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": state[\"documents\"], \"question\": new_question}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"使用网络搜索工具\"\"\"\n",
    "    print(\"Searching the web...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([doc['content'] for doc in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    return {\"documents\": [web_results], \"question\": question}\n",
    "\n",
    "### 构建边\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"route: 根据问题分配不同的处理方式\"\"\"\n",
    "    print(\"Routing question...\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.route == 'web_search':\n",
    "        print(\"Routing to web search...\")\n",
    "        return \"web_search\"\n",
    "    elif source.route == 'vectorstore':\n",
    "        print(\"Routing to vectorstore...\")\n",
    "        return \"retrieve\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"判断是否生成答案\"\"\"\n",
    "    print(\"Deciding to generate...\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "    if not filtered_documents:\n",
    "        # 所有检索的文档都不好 重修将问题转换为新问题 再次检索\n",
    "        print(\"No relevant documents. Transforming query...\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"Generating answer...\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generate_v_documents_and_question(state):\n",
    "    \"\"\"生成回答，并根据回答，首先进行幻觉评分，然后进行结果评分\"\"\"\n",
    "    print(\"Grading generation...\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        print(\"Grading answer...\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"answer\": generation})\n",
    "        if score.binary_score < 3:\n",
    "           print(f\"{score.binary_score} is a low score. generation does not address question...\")\n",
    "           return \"not useful\"\n",
    "        else:\n",
    "            print(f\"{score.binary_score} is a high score. generation is good...\")\n",
    "            return \"useful\"\n",
    "    else:\n",
    "        print(\"Generation not grounded in facts\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "adaptive_workflow = StateGraph(GraphState)\n",
    "\n",
    "# 定义节点\n",
    "adaptive_workflow.add_node(\"web_search\", web_search)\n",
    "adaptive_workflow.add_node(\"retrieve\", retrieve)\n",
    "adaptive_workflow.add_node(\"grade_documents\", grade_documents)\n",
    "adaptive_workflow.add_node(\"generate\", generate)\n",
    "adaptive_workflow.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "# 定义图\n",
    "adaptive_workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "adaptive_workflow.add_edge(\"web_search\", \"generate\")\n",
    "adaptive_workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "adaptive_workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"transform_query\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "adaptive_workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generate_v_documents_and_question,\n",
    "    {   \n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "app = adaptive_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing question...\n",
      "Routing to web search...\n",
      "Searching the web...\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "Generating answer...\n",
      "[Document(page_content=\"The Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\nHere is how Ryan Poles and Matt Eberflus did with their selections for the Chicago Bears 2024 NFL Draft: Round 1 • Pick 1 (1) • QB Caleb Williams USC. Round 1 • Pick 9 (9) • WR Rome Odunze ...\\nChicago Bears Draft Overview. 2023 record: 7-10. Fourth in NFC North; missed playoffs. Bears 2024 draft picks (4): Round 1, pick 1 (from the Panthers in the D.J. Moore/2023 No. 1 overall pick ...\\nThe Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections — quarterback Caleb Williams (No. 1 ...\\nApr 24, 2024. The Beast, Dane Brugler's expansive guide to the NFL Draft, is here. The Chicago Bears have the first and ninth picks in the NFL Draft when Round 1 begins April 25 in Detroit. The ...\")]\n",
      "<class 'list'>\n",
      "Grading generation...\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "Grading answer...\n",
      "4 is a high score. generation is good...\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('Caleb Williams, a quarterback from USC, was selected by the Bears with the '\n",
      " 'No. 1 pick in the 2024 NFL Draft. He was considered the top prospect in a '\n",
      " 'draft class filled with talented quarterbacks.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What player at the Bears expected to draft first in the 2024 NFL draft?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing question...\n",
      "Routing to vectorstore...\n",
      "Retrieving documents...\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "Grading documents...\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "Document relevant\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "Document relevant\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "Document not relevant\n",
      "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n",
      "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "Document relevant\n",
      "Deciding to generate...\n",
      "Generating answer...\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "Generating answer...\n",
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.')]\n",
      "<class 'list'>\n",
      "Grading generation...\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "Grading answer...\n",
      "4 is a high score. generation is good...\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('The types of agent memory are short-term memory, long-term memory, and '\n",
      " 'sensory memory.')\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value [\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(nodes={'__start__': Node(id='__start__', name='__start__', data=<class 'pydantic.v1.main.LangGraphInput'>, metadata=None), 'web_search': Node(id='web_search', name='web_search', data=web_search(recurse=True), metadata=None), 'retrieve': Node(id='retrieve', name='retrieve', data=retrieve(recurse=True), metadata=None), 'grade_documents': Node(id='grade_documents', name='grade_documents', data=grade_documents(recurse=True), metadata=None), 'generate': Node(id='generate', name='generate', data=generate(recurse=True), metadata=None), 'transform_query': Node(id='transform_query', name='transform_query', data=transform_query(recurse=True), metadata=None), 'route_question': Node(id='route_question', name='route_question', data=route_question(recurse=True), metadata=None), '__end__': Node(id='__end__', name='__end__', data=<class 'pydantic.v1.main.LangGraphOutput'>, metadata=None)}, edges=[Edge(source='__start__', target='route_question', data=None, conditional=False), Edge(source='retrieve', target='grade_documents', data=None, conditional=False), Edge(source='web_search', target='generate', data=None, conditional=False), Edge(source='__start__', target='web_search', data=None, conditional=True), Edge(source='__start__', target='retrieve', data=None, conditional=True), Edge(source='grade_documents', target='generate', data=None, conditional=True), Edge(source='grade_documents', target='transform_query', data=None, conditional=True), Edge(source='generate', target='generate', data='not supported', conditional=True), Edge(source='generate', target='__end__', data='useful', conditional=True), Edge(source='generate', target='transform_query', data='not useful', conditional=True)])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrective RAG(CRAG)\n",
    "\n",
    "![img\\CRAG.png](img/CRAG.png)\n",
    "\n",
    "* 文档相关性检查：在生成答案之前，首先会检查检索到的文档是否至少有一个文档达到了相关性阈值。如果有文档满足该条件，系统会继续进行答案生成。\n",
    "\n",
    "* 知识精炼：在生成之前，会对检索到的文档进行知识精炼，将文档划分为多个“知识片段”。这些片段代表文档中的不同知识点或信息单元。\n",
    "\n",
    "* 自我评分与过滤：系统会对每个知识片段进行评分，过滤掉那些与查询无关的片段。只有通过评分的相关片段才会被用于答案生成。\n",
    "\n",
    "* 补充信息检索：如果所有文档的相关性评分都低于阈值，或者评分器无法确定文档的相关性，系统将主动寻求额外的数据源进行补充检索，通常是通过网络搜索来获取更多相关信息。\n",
    "\n",
    "该部分不详细解释了，感觉就是之前如果没有合适的结果，重新构建query 现在是拓展相关知识，上网搜索 核心没有改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Agent\n",
    "将以下技术合并在一起 构建了一个RAG agent model\n",
    "* Routing: Adaptive RAG, Route questions to different retrieval approaches\n",
    "* Fall back: Corrective RAG 如果docs和query不相关 使用 web search\n",
    "* Self-correction: self-RAG 检查回答有没有幻觉和是否解决了问题\n",
    "\n",
    "![img\\RAG_Agent_all.png](img/RAG_Agent_all.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "docs = [WebBaseLoader(url,bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")),)).load() \n",
    "        for url in urls]\n",
    "docs_list = [sublist for doc in docs for sublist in doc]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
    "doc = text_splitter.split_documents(docs_list)\n",
    "embed = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(doc, embedding=embed)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\"),\n",
    "        (\"human\", \"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: {question} \\n assistant:\")\n",
    "    ]\n",
    ")\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"document\": doc_text, \"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-term memory provides the agent with the ability to retain and recall vast amounts of information over extended periods by using an external vector store for fast retrieval. The memory stream is an external database that records a detailed list of the agent's experiences in natural language. This long-term memory serves as an external vector store that the agent can access quickly during query time.\n"
     ]
    }
   ],
   "source": [
    "### 生成\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise\"\"\"),\n",
    "        ('human',\"\"\"Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \"\"\")\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"question\": question, \"context\": format_docs(docs)})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader 幻觉评分\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation.\"\"\"),\n",
    "        (\"human\", \"\"\"Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}\\n assistant:\"\"\")\n",
    "    ]\n",
    ")\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader \n",
    "### 不需要with_structured_output 在prompt中使用system给予对应的提示\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\"),\n",
    "        (\"human\", \"\"\"Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question}  \\n assistant:\"\"\")\n",
    "    ]\n",
    ")\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are an expert at routing a user question to a vectorstore or web search. The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explaination. \"\"\"),\n",
    "        (\"human\", \"Question to route: {question}\\n assistant:\")\n",
    "    ]\n",
    ")\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_text = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 构建图\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"Retrieving documents...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    return {\"documents\": docs, \"question\": question}\n",
    "\n",
    "def generation(state):\n",
    "    print(\"Generating answer...\")\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"question\": question, \"context\": format_docs(docs)})\n",
    "    return {\"documents\": docs, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"Grading documents...\")\n",
    "    question = state[\"question\"]\n",
    "    print(f'question: {question}')\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for doc in documents:\n",
    "        print(doc.page_content)\n",
    "        score = retrieval_grader.invoke({\"document\": doc.page_content, \"question\": question})\n",
    "        grade = score['score']\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"Document relevant\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"Document not relevant\")\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"Searching the web...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([doc['content'] for doc in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "## 构建边\n",
    "def route_question(state):\n",
    "    print(\"Routing question...\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source['datasource'] == 'web_search':\n",
    "        print(\"Routing to web search...\")\n",
    "        return \"websearch\"\n",
    "    elif source['datasource'] == 'vectorstore':\n",
    "        print(\"Routing to vectorstore...\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    print(\"Assessing graded documents...\")\n",
    "    web_search = state[\"web_search\"]\n",
    "    if web_search == \"Yes\":\n",
    "        print(\"No relevant documents. Transforming query...\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        print(\"Generating answer...\")\n",
    "        return \"generate\"\n",
    "\n",
    "### 构建条件边\n",
    "def grade_generate_v_documents_and_question(state):\n",
    "    print(\"Checking hallucinations...\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "    if grade == \"yes\":\n",
    "        print(\"--Decision: generation is grounded in documents--\")\n",
    "        print(\"Grading answer...\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(f\"--Decision: generation is useful--\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(f\"--Decision: generation is not useful--\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--Decision: generation is not grounded in documents--\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generation)\n",
    "\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"websearch\": \"websearch\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generate_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing question...\n",
      "Routing to vectorstore...\n",
      "Retrieving documents...\n",
      "'Finished running: retrieve:'\n",
      "Grading documents...\n",
      "question: What are the types of agent memory?\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "Document relevant\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "Document relevant\n",
      "Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "Document not relevant\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Document not relevant\n",
      "Assessing graded documents...\n",
      "No relevant documents. Transforming query...\n",
      "'Finished running: grade_documents:'\n",
      "Searching the web...\n",
      "'Finished running: websearch:'\n",
      "Generating answer...\n",
      "Checking hallucinations...\n",
      "--Decision: generation is grounded in documents--\n",
      "Grading answer...\n",
      "--Decision: generation is useful--\n",
      "'Finished running: generate:'\n",
      "('The types of agent memory include Long-term memory, which allows the agent '\n",
      " 'to retain and recall infinite information over extended periods by '\n",
      " 'leveraging an external vector store and fast retrieval. Another type is '\n",
      " 'Short-term memory (STM), which is a data structure synthesized before '\n",
      " 'runtime by replacing relevant variables in a prompt template with '\n",
      " 'information retrieved from Long-term memory. Agents can be categorized into '\n",
      " 'five types: Simple Reflex agents, Model-based Reflex agents, Goal-based '\n",
      " 'agents, Utility-based agents, and Learning agents.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing question...\n",
      "Routing to web search...\n",
      "Searching the web...\n",
      "'Finished running: websearch:'\n",
      "Generating answer...\n",
      "Checking hallucinations...\n",
      "--Decision: generation is grounded in documents--\n",
      "Grading answer...\n",
      "--Decision: generation is useful--\n",
      "'Finished running: generate:'\n",
      "('The Bears are expected to draft USC quarterback Caleb Williams first overall '\n",
      " 'in the NFL draft. The 2024 NFL draft is set to begin on Thursday, April 25, '\n",
      " 'with the Bears holding the first pick. Caleb Williams, a star quarterback '\n",
      " 'from USC and the 2022 Heisman Trophy winner, is the likely choice for the '\n",
      " \"Bears' first selection.\")\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Who are the Bears expected to draft first in the NFL draft?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(nodes={'__start__': Node(id='__start__', name='__start__', data=<class 'pydantic.v1.main.LangGraphInput'>, metadata=None), 'websearch': Node(id='websearch', name='websearch', data=websearch(recurse=True), metadata=None), 'retrieve': Node(id='retrieve', name='retrieve', data=retrieve(recurse=True), metadata=None), 'grade_documents': Node(id='grade_documents', name='grade_documents', data=grade_documents(recurse=True), metadata=None), 'generate': Node(id='generate', name='generate', data=generate(recurse=True), metadata=None), '__end__': Node(id='__end__', name='__end__', data=<class 'pydantic.v1.main.LangGraphOutput'>, metadata=None)}, edges=[Edge(source='retrieve', target='grade_documents', data=None, conditional=False), Edge(source='websearch', target='generate', data=None, conditional=False), Edge(source='__start__', target='websearch', data=None, conditional=True), Edge(source='__start__', target='retrieve', data='vectorstore', conditional=True), Edge(source='grade_documents', target='generate', data=None, conditional=True), Edge(source='grade_documents', target='websearch', data=None, conditional=True), Edge(source='generate', target='generate', data='not supported', conditional=True), Edge(source='generate', target='__end__', data='useful', conditional=True), Edge(source='generate', target='websearch', data='not useful', conditional=True)])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
